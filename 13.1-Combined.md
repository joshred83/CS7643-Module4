# Quiz: Generative Models - Deep Dive

### Question 1 (Multi-Select)
Which of the following accurately describe the role and goals of generative models in unsupervised learning?

- [ ] They only classify input data into known categories.
- [ ] They estimate the probability distribution over the input space.
- [ ] They can be used to generate new, artificial samples resembling the data.
- [ ] They are used for tasks such as density estimation.
- [ ] They require labeled data to function effectively.

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Estimate probability distribution, ✅ Generate new samples, ✅ Used for density estimation  
**Explanation:**  
Generative models do **not require labeled data** and **do more than classification**.

> "Here we take a probabilistic view of unsupervised learning and try to estimate the probability distribution over the input space."  
> "In this lesson, we'll focus on Density Estimation... we may want to just have the ability to generate samples from this distribution..."
</details>

---

### Question 2 (Multiple Choice)
Which traditional method was used for density estimation before the resurgence of deep learning, despite its limitations with high-dimensional data?

- [ ] Naive Bayes classifiers
- [ ] Support Vector Machines
- [ ] Gaussian Mixture Models
- [ ] Linear Discriminant Analysis

<details>
<summary>Show Answer</summary>

**Correct Answer:** ✅ Gaussian Mixture Models  
**Explanation:**  
GMMs were traditionally used, though they struggle with high-dimensional input spaces.

> "For example, Gaussian mixture models also produce some estimate of the probability distribution over the input space. However, these methods have severe deficiencies when the input is very high dimensional."
</details>

---

### Question 3 (True/False)
Generative models model the conditional probability of labels given the input data.

- [ ] True
- [ ] False

<details>
<summary>Show Answer</summary>

**Correct Answer:** ❌ False  
**Explanation:**  
Discriminative models model \( P(y|x) \); generative models model \( P(x) \).

> "Discriminative models, model the conditional distribution probability of the label given the input... Generative models, on the other hand, model the distribution over the input space."
</details>

---

### Question 4 (Multi-Select)
What are key challenges associated with modeling the joint distribution of high-dimensional data in generative models?

- [ ] High computational complexity
- [ ] Intractability of the exact probability distribution
- [ ] Lack of any theoretical basis
- [ ] Requirement of assumptions or simplifications

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ High computational complexity, ✅ Intractability, ✅ Need for simplification  
**Explanation:**  
Modeling \( P(x) \) directly is challenging without simplifying assumptions.

> "This is a very intractable and hard thing to do. And so we'll have to make various assumptions or simplifications in order to make this feasible."
</details>

---

### Question 5 (Concept Application)
You are asked to design a generative model for a new high-dimensional image dataset. What strategies from the transcript are likely to help in tackling the complexity?

- [ ] Use deep learning architectures that reduce dimensionality
- [ ] Use rule-based symbolic reasoning
- [ ] Leverage embeddings to find low-dimensional representations
- [ ] Avoid simplifications to maintain full generality

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Use deep architectures, ✅ Use embeddings  
**Explanation:**  
The transcript emphasizes dimensionality reduction using neural networks.

> "Deep learning is very good at learning features that extract meaningful information in a low dimensional embedding from high dimensional data."
</details>

---

### Question 6 (Multiple Choice)
Which of the following statements best captures the distinction between discriminative and generative models?

- [ ] Discriminative models generate new samples from the data distribution.
- [ ] Generative models are always more accurate than discriminative ones.
- [ ] Discriminative models estimate \( P(y|x) \), while generative models estimate \( P(x) \).
- [ ] Generative models require supervised data, while discriminative models do not.

<details>
<summary>Show Answer</summary>

**Correct Answer:** ✅ Discriminative: \( P(y|x) \), Generative: \( P(x) \)  
**Explanation:**  
This is a central distinction made in the lesson.

> "Discriminative models, model the conditional distribution probability of the label given the input... Generative models... model the distribution over the input space."
</details>

---

### Question 7 (Multi-Select)
According to the transcript, why is deep learning suitable for generative modeling of high-dimensional data?

- [ ] Deep learning methods can extract meaningful low-dimensional features.
- [ ] Neural networks can model complex distributions with sufficient flexibility.
- [ ] They inherently encode explicit probabilistic rules.
- [ ] They eliminate the need for any data preprocessing.

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Low-dim features, ✅ Flexible modeling  
**Explanation:**  
Deep learning reduces dimensionality and provides flexibility in modeling.

> "Deep learning is very good at learning features that extract meaningful information in a low dimensional embedding..."  
> "Just, like discriminative models we can have a parametric approximation of this distribution."
</details>

---

### Question 8 (True/False)
The ability to generate new data samples is a core feature of generative models.

- [ ] True
- [ ] False

<details>
<summary>Show Answer</summary>

**Correct Answer:** ✅ True  
**Explanation:**  
Generating samples is one of the core applications.

> "We may want to just have the ability to generate samples from this distribution, that is actually generate artificial examples..."
</details>

---

### Question 9 (Multiple Choice)
Which principle is often used to optimize the parameters of generative models?

- [ ] Cross-entropy minimization
- [ ] Maximum likelihood estimation
- [ ] Mean squared error
- [ ] KL divergence minimization

<details>
<summary>Show Answer</summary>

**Correct Answer:** ✅ Maximum likelihood estimation  
**Explanation:**  
MLE is the optimization principle used for learning generative models.

> "We can have a set of parameterized models \( p(x, \theta) \)... use the principle of maximum likelihood
</details>