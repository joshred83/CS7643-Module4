# Answer Key: Algorithms for Solving MDPs

---

### Question 1  
**Correct Answer:** True  
**Explanation:**  
The optimal value function is defined as the max over all Q-values at that state.  
> "The first says that the optimal value at a state is the same as the max Q value over possible actions at that state."

---

### Question 2  
**Correct Answer:** A recursive definition of the optimal value function  
**Explanation:**  
The Bellman equation defines value functions recursively based on future values.  
> "Taking a closer look at the definition of the optimal Q function, we will now try to rewrite it recursively..."  
> "The recursive Bellman equation derived so far will form the basis for... value iteration."

---

### Question 3  
**Correct Answer:** To iteratively converge on the optimal value function  
**Explanation:**  
Value iteration repeatedly applies Bellman updates until convergence.  
> "The central idea is to update this vector at each iteration by repeatedly applying this recursive Bellman equation until convergence."

---

### Question 4  
**Correct Answers:** A series of value vectors \( V_0, V_1, \ldots \), Updates using the recursive Bellman equation, Time complexity that scales with the product of number of states and actions  
**Explanation:**  
The algorithm builds up sequences of value vectors, applies Bellman updates, and has quadratic time complexity.  
> "This update will produce a sequence of vectors V0, V1, and so on..."  
> "...by repeatedly applying this recursive Bellman equation..."  
> "Each iteration of this algorithm will have a time complexity of order of n square m..."

---

### Question 5  
**Correct Answer:** False  
**Explanation:**  
Although policy iteration involves a policy evaluation step, it often converges faster.  
> "...the policy converges to pi star much sooner than the value converges to V of pi star, thus requiring fewer iterations."

---

### Question 6  
**Correct Answer:** Q iteration operates directly on the Q-function  
**Explanation:**  
Q iteration works by updating Q-values directly instead of value vectors.  
> "We can derive an update rule for Q functions, which will form the basis of the Q iteration algorithm."

---

### Question 7  
**Correct Answer:** True  
**Explanation:**  
Policy iteration alternates between computing value estimates and performing greedy updates.  
> "The policy iteration algorithm involves two parts... compute V pi... then greedily update the policy."

---

### Question 8  
**Correct Answers:** They require summing over all states and actions, The number of states in environments like Atari or chess is extremely large  
**Explanation:**  
Dynamic programming is computationally expensive and not feasible for large state/action spaces.  
> "...time complexity of one iteration update..."  
> "...chess... our lower bound being 10 to the power 420 states. And for Atari Games... the number of such images is also exponentially large."

---

### Question 9  
**Correct Answer:** To choose the action that maximizes the expected value at each state  
**Explanation:**  
The greedy step in policy improvement involves selecting the action with maximum expected value.  
> "This greedy step involves picking the action that maximizes the value obtained at all states..."

---

### Question 10  
**Correct Answer:** True  
**Explanation:**  
The lecture makes clear that these algorithms are built upon the Bellman optimality equations.  
> "We derived the recursive Bellman optimality equations that form the backbone of the three dynamic programming algorithms..."

