# Answer Key: Markov Decision Processes (MDPs)

---

### Question 1  
**Correct Answer:** To serve as the foundational model for reinforcement learning  
**Explanation:**  
MDPs are introduced as the theoretical framework behind reinforcement learning.  
> "MDPs can be thought of as a theoretical framework underlying RL."

---

### Question 2  
**Correct Answers:** A reward function, A discount factor, A transition probability distribution  
**Explanation:**  
The MDP tuple includes states, actions, rewards, transitions, and a discount factor.  
> "An MDP is defined as a tuple of five items. S... A... R is the reward distribution... T is the transition probability distribution... Gamma is a discount factor..."

---

### Question 3  
**Correct Answer:** False  
**Explanation:**  
The Markov property asserts dependence only on the current state and action, not the full history.  
> "The distribution of possible next states given state s, and action a. Does not depend on any of the previous states or actions..."

---

### Question 4  
**Correct Answer:** Samples from the reward and transition distributions  
**Explanation:**  
Agents observe samples, not full knowledge, of transitions and rewards.  
> "The transition distribution and the reward distribution are both not known. Instead, only samples from these distributions are observed by the agent..."

---

### Question 5  
**Correct Answers:** A deterministic policy maps states to actions, A stochastic policy defines a probability distribution over actions, A deterministic policy stores one action per state  
**Explanation:**  
Deterministic and stochastic policies differ by how they assign actions: one fixed action vs. a distribution.  
> "A deterministic policy is defined as a mapping from states to actions... A stochastic policy is defined as a probability distribution of actions given a state..."

---

### Question 6  
**Correct Answer:** How much weight is given to future rewards  
**Explanation:**  
The discount factor gamma adjusts the importance of rewards over time.  
> "The discount factor gamma lies between 0 and 1... implying that the rewards at earlier timestamps, are given more weight..."

---

### Question 7  
**Correct Answer:** True  
**Explanation:**  
Low gamma places more emphasis on near-term rewards.  
> "...a lower value of gamma, prioritizes the lower rewarding state at the right endpoint."

---

### Question 8  
**Correct Answers:** State-value function (V-function), State-action value function (Q-function)  
**Explanation:**  
Both value functions are introduced explicitly to evaluate policies.  
> "A value function... is a prediction of discounted sum of future rewards."  
> "A state action value function or a Q-function... informs us of how good is taking a particular action at a state."

---

### Question 9  
**Correct Answer:** The agent tends to prefer shorter or riskier paths to reach absorbing states  
**Explanation:**  
As negative rewards increase, the policy changes to prefer quicker or alternative outcomes.  
> "...as this constant reward decreases to -0.4... the optimal policy... takes the riskier shorter path..."  
> "Further, decreasing this constant to -2... the optimal policy now prefers the -1 absorbing state..."

---

### Question 10  
**Correct Answer:** True  
**Explanation:**  
The Q-function assesses the expected future reward for an action-state pair.  
> "The Q-function for a policy... is the expected sum of discounted rewards... after taking action a at state s."

