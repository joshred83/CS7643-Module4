# Quiz: Unsupervised and Self-Supervised Learning

---

### Question 1 (True/False)  
Unsupervised learning requires labels to compute the loss functions used for training.

- True  
- False  

---

### Question 2 (Multiple Choice)  
What is the primary purpose of autoencoders in unsupervised learning?

- To classify input data using pseudo-labels  
- To reconstruct labeled outputs from input data  
- To extract low-dimensional representations that capture important features  
- To optimize for high-resolution image synthesis  

---

### Question 3 (Multiple Select)  
Which of the following are traditional tasks associated with unsupervised learning?

- Clustering  
- Dimensionality reduction  
- Reinforcement signal propagation  
- Density estimation  

---

### Question 4 (Multiple Choice)  
What does the “bottleneck” in an autoencoder architecture encourage?

- Overfitting to high-dimensional features  
- Learning trivial identity functions  
- Compression of inputs into meaningful low-dimensional embeddings  
- Increased accuracy through deep convolutional filters  

---

### Question 5 (True/False)  
Autoencoders can be trained without any labels, using only the input data.

- True  
- False  

---

### Question 6 (Multiple Select)  
What are characteristics of an effective feature space learned via unsupervised learning?

- High density clusters for semantically similar inputs  
- Widely dispersed examples regardless of semantic meaning  
- Low-density regions separating conceptually distinct inputs  
- Spatial locality preserved through convolutional filters  

---

### Question 7 (Multiple Choice)  
In clustering-based unsupervised learning pipelines, what are pseudo-labels?

- Randomly assigned labels for evaluation  
- Labels assigned by a pretrained supervised classifier  
- Cluster assignments used as training targets for backpropagation  
- Binary indicators for supervised similarity  

---

### Question 8 (Multiple Select)  
Which are potential issues when clustering is used to generate pseudo-labels?

- Empty clusters  
- Trivial solutions where all items are grouped into one cluster  
- Perfectly balanced partitions that prevent overfitting  
- Inability to use backpropagation through clustering  

---

### Question 9 (True/False)  
Surrogate tasks are designed to train models using freely available labels from data transformations.

- True  
- False  

---

### Question 10 (Multiple Choice)  
Why is predicting image rotation considered a useful surrogate task?

- It trains the network to detect object boundaries precisely  
- It forces the model to recognize semantic content of images  
- It directly improves classification performance on downstream tasks  
- It encodes time-based sequence relationships  

---

### Question 11 (Multiple Select)  
Which of the following are common surrogate tasks for self-supervised learning?

- Colorization  
- Jigsaw puzzle solving  
- Rotation prediction  
- GAN-based style transfer  

---

### Question 12 (True/False)  
The quality of learned representations is typically evaluated by training a complex nonlinear classifier on top of the encoder.

- True  
- False  

---

### Question 13 (Multiple Choice)  
What is the goal of instance discrimination in contrastive learning?

- Maximize diversity of the input images  
- Align representations of augmentations of the same image while separating others  
- Match latent codes across different modalities  
- Use nearest neighbors for label propagation  

---

### Question 14 (Multiple Select)  
Which techniques improve the efficiency of instance discrimination training?

- Using mini-batch negatives  
- Maintaining a memory bank of encoded features  
- Sampling only hard negatives manually  
- Using an exponential moving average to stabilize key encoders  

---

### Question 15 (Multiple Choice)  
What is a key advantage of self-supervised representations learned via contrastive learning?

- They generalize poorly but train quickly  
- They replace labeled data with GAN-generated features  
- They enable competitive performance on downstream tasks even with frozen encoders  
- They require significantly more parameters than supervised models  
