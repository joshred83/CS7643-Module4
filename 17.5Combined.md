# Quiz: Policy Gradients and Actor-Critic Methods



### Question 1 (True/False)  
Policy-based reinforcement learning methods directly optimize a parameterized policy.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** True  
**Explanation:**  
Policy-based methods directly parameterize and optimize a policy.  
> "Policy based methods on the other hand directly parameterize a policy and optimize it to maximize returns."
</details>

---



### Question 2 (Multi-Select)  
Which of the following accurately describe differences between value-based and policy-based reinforcement learning methods? (Select all that apply)

- Value-based methods optimize value functions, while policy-based methods optimize policies directly
- Policy-based methods can naturally handle continuous action spaces, unlike basic value-based methods
- Value-based methods typically use epsilon-greedy exploration, while policy-based methods can learn stochastic policies
- Policy-based methods learn a Q-function before training a policy
- Value-based methods indirectly derive policies (e.g., greedy with respect to Q), while policy-based methods represent policies explicitly
- Value-based methods require discretization of continuous action spaces

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Optimize different targets (values vs policies), ✅ Different handling of continuous actions, ✅ Different exploration approaches, ✅ Indirect vs explicit policy representation, ✅ Different discretization requirements  
**Explanation:**  
Value-based and policy-based methods differ in several fundamental ways.  
> "Among the types of methods using RL value based methods learn q functions... Policy based methods... directly parameterize a policy..."
> "Policy-based methods have some advantages... they can naturally handle continuous action spaces."
> "Value-based methods derive policies indirectly, typically by being greedy with respect to the learned value function."
</details>

---



### Question 3 (Multiple Select)  
Which of the following are components of the REINFORCE algorithm?

- Sampling trajectories using the current policy  
- Computing gradients with respect to a value function  
- Applying the log-derivative trick to compute the gradient  
- Using a fixed reward function for supervised updates

<details>
<summary>Show Answer</summary>

**Correct Answers:** Sampling trajectories using the current policy, Applying the log-derivative trick to compute the gradient  
**Explanation:**  
REINFORCE involves sampling data and using the log-derivative trick to compute gradients.  
> "We simply need to collect a small batch of trajectories using the current pi theta..."  
> "...in order to apply a relation known as the log derivative trick..."
</details>

---



### Question 4 (Multi-Select)  
Which of the following accurately describe the purposes and benefits of using the log-derivative trick in policy gradient methods? (Select all that apply)

- To simplify the integral of the gradient of a probability density
- To convert a complex gradient computation into an expectation that can be sampled
- To avoid computing derivatives of transition dynamics
- To enable gradient estimation without knowing the environment model
- To facilitate policy optimization via stochastic gradient ascent
- To ensure the Q-function remains differentiable

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Simplify integral of gradient, ✅ Convert to expectation for sampling, ✅ Avoid derivatives of dynamics, ✅ Enable model-free gradient estimation, ✅ Facilitate stochastic gradient ascent  
**Explanation:**  
The log-derivative trick has several important benefits in policy gradient algorithms.  
> "...we can write the expectation as an integral... and apply a relation known as the log derivative trick..."
> "This allows us to express the gradient in terms of an expectation, which we can then estimate by sampling."
> "The policy gradient theorem provides a way to compute policy gradients without knowing the dynamics of the environment."
</details>

---



### Question 5 (True/False)  
In REINFORCE, the gradient update increases the likelihood of action sequences that result in higher rewards.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** True  
**Explanation:**  
Gradient updates push the probability of successful actions higher.  
> "...the gradient update will push the probability of the chosen actions to be either higher or lower depending on the sign and magnitude of the total reward..."
</details>

---



### Question 6 (Multiple Select)  
What are some limitations of REINFORCE as discussed in the lecture?

- It cannot compute gradients for deterministic policies  
- It suffers from high variance in gradient estimates  
- It requires knowledge of transition probabilities  
- It does not assign credit to individual actions within a trajectory

<details>
<summary>Show Answer</summary>

**Correct Answers:** It suffers from high variance in gradient estimates, It does not assign credit to individual actions within a trajectory  
**Explanation:**  
REINFORCE suffers from variance and lacks fine-grained credit assignment.  
> "...we are left with a coarse level feedback for the entire sequence..."  
> "...New variants of this algorithm have been proposed that aim to reduce the variance..."
</details>

---



### Question 7 (Multi-Select)  
Which of the following accurately describe the roles and properties of baselines in policy gradient algorithms? (Select all that apply)

- They reduce variance in gradient estimation
- They preserve the expected value of the gradient
- They can be state-dependent functions
- They are often implemented as value function approximations
- They ensure deterministic transitions
- They help address the credit assignment problem in long trajectories

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Reduce variance, ✅ Preserve expected gradient, ✅ Can be state-dependent, ✅ Often implemented as value functions, ✅ Help with credit assignment  
**Explanation:**  
Baselines serve several important purposes in policy gradient methods.  
> "...subtracting some baseline... will preserve the mean... while possibly reducing the variance..."
> "We can use a state-dependent baseline such as a value function approximation."
> "A good baseline helps distinguish which actions were actually better than average, addressing the credit assignment problem."
</details>

---



### Question 8 (True/False)  
Subtracting a baseline that does not depend on actions changes the mean of the policy gradient estimate.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Subtracting a baseline that does not depend on action preserves the mean.  
> "...subtracting some baseline... will preserve the mean of the gradient expectation..."
</details>

---



### Question 9 (Multi-Select)  
Which of the following accurately describe scaling factors and signal reductions used in different actor-critic variant algorithms? (Select all that apply)

- The advantage function (Q minus V) is used in Advantage Actor-Critic
- TD error can be used as an unbiased estimate of the advantage
- State-value functions serve as baselines for variance reduction
- A combination of n-step returns and value estimates can be used
- Importance sampling weights are applied when using off-policy data
- Raw rewards without any baseline are optimal for stable training

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Advantage function (Q minus V), ✅ TD error as advantage estimate, ✅ State-value functions as baselines, ✅ Combination of n-step returns and values, ✅ Importance sampling for off-policy  
**Explanation:**  
Actor-critic methods use various signals and scaling factors to improve performance.  
> "...advantage actor-critic that substitutes the reward with the advantage... defined as the Q function minus the V function."
> "TD error can be used as an unbiased sample of the advantage function."
> "State-value functions provide effective baselines that reduce variance while preserving the expected gradient."
</details>

---



### Question 10 (Multiple Select)  
Which elements are used in the actor-critic algorithm?

- A learned Q-function  
- Policy gradient updates  
- Advantage estimates  
- A reward baseline derived from the policy’s V-function

<details>
<summary>Show Answer</summary>

**Correct Answers:** A learned Q-function, Policy gradient updates, Advantage estimates, A reward baseline derived from the policy’s V-function  
**Explanation:**  
Actor-Critic methods include all of these elements as discussed.  
> "...actor-critic algorithm that replaces rewards with the Q function..."  
> "...advantage actor-critic... defined as the Q function minus the V function."
</details>

