# Answer Key: Unsupervised and Self-Supervised Learning

---

### Question 1  
**Correct Answer:** False  
**Explanation:**  
Unsupervised learning does not rely on labels; it uses patterns in the data.  
> "We assume we have no labels, all we have are unlabeled examples..."

---

### Question 2  
**Correct Answer:** To extract low-dimensional representations that capture important features  
**Explanation:**  
Autoencoders compress data into lower-dimensional representations for reconstruction.  
> "...we're trying to compress the image or any other data through some bottleneck..."

---

### Question 3  
**Correct Answers:** Clustering, Dimensionality reduction, Density estimation  
**Explanation:**  
These are core classical tasks under unsupervised learning.  
> "...tasks such as clustering or density estimation..."  
> "...use an autoencoder to perform dimensionality reduction..."

---

### Question 4  
**Correct Answer:** Compression of inputs into meaningful low-dimensional embeddings  
**Explanation:**  
The bottleneck enforces feature learning through constrained representation.  
> "...compress the image... and then decompress it again..."

---

### Question 5  
**Correct Answer:** True  
**Explanation:**  
Autoencoders learn from reconstruction loss, not labels.  
> "...we train the autoencoder to reconstruct the original image..."

---

### Question 6  
**Correct Answers:** High density clusters for semantically similar inputs, Low-density regions separating conceptually distinct inputs  
**Explanation:**  
Effective representations cluster similar inputs and separate dissimilar ones.  
> "...samples that are semantically similar are close together..."  
> "...semantically dissimilar examples are far apart..."

---

### Question 7  
**Correct Answer:** Cluster assignments used as training targets for backpropagation  
**Explanation:**  
Clustering assigns pseudo-labels which are then used as targets.  
> "...use clustering algorithms to generate pseudo-labels for the unlabeled data..."

---

### Question 8  
**Correct Answers:** Empty clusters, Trivial solutions where all items are grouped into one cluster  
**Explanation:**  
These are known risks in clustering-based training.  
> "...there may be empty clusters or trivial solutions where all data collapse to one cluster..."

---

### Question 9  
**Correct Answer:** True  
**Explanation:**  
Surrogate tasks create labels from data transformations.  
> "...a surrogate task... where we try to predict the rotation... even though the original image has no label."

---

### Question 10  
**Correct Answer:** It forces the model to recognize semantic content of images  
**Explanation:**  
Rotation prediction forces semantic awareness.  
> "...predict which of the four rotations... the neural network must understand whether there is an upright dog..."

---

### Question 11  
**Correct Answers:** Colorization, Jigsaw puzzle solving, Rotation prediction  
**Explanation:**  
These are classic self-supervised tasks.  
> "...predict the correct color... jigsaw puzzles... rotation prediction..."

---

### Question 12  
**Correct Answer:** False  
**Explanation:**  
Representations are evaluated using a simple linear classifier to avoid confounding effects.  
> "We will train a linear classifier on top of the features... and test the accuracy..."

---

### Question 13  
**Correct Answer:** Align representations of augmentations of the same image while separating others  
**Explanation:**  
Instance discrimination aims to align augmentations and separate distinct inputs.  
> "...make the representations of the augmentations of the same image close together... other examples farther apart..."

---

### Question 14  
**Correct Answers:** Using mini-batch negatives, Maintaining a memory bank of encoded features, Using an exponential moving average to stabilize key encoders  
**Explanation:**  
All of these are techniques used to improve training in contrastive learning.  
> "...use mini-batch negatives... use a memory bank... exponential moving average of the encoder weights..."

---

### Question 15  
**Correct Answer:** They enable competitive performance on downstream tasks even with frozen encoders  
**Explanation:**  
Contrastive methods achieve strong results with fixed feature extractors.  
> "...you get extremely strong performance, even when you freeze the features and only train a linear classifier..."

