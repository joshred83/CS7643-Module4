# Quiz: Algorithms for Solving MDPs

---

### Question 1 (True/False)
The optimal value function \( V^* \) for a state is defined as the maximum over all actions of the corresponding optimal Q-values \( Q^*(s, a) \).

- True  
- False  

---

### Question 2 (Multiple Choice)
What does the Bellman equation represent in the context of MDPs?

- A way to estimate sample variance in policy rewards  
- A recursive definition of the optimal value function  
- A replacement for stochastic policies in reinforcement learning  
- A visualization technique for state transitions  

---

### Question 3 (Multiple Choice)
What is the goal of the value iteration algorithm?

- To create a new reward function based on policy gradients  
- To approximate the policy gradient through finite differences  
- To iteratively converge on the optimal value function  
- To generate visualizations of the policy graph  

---

### Question 4 (Multiple Select)
Which of the following are components or results of value iteration?

- A series of value vectors \( V_0, V_1, \ldots \)  
- Updates using the recursive Bellman equation  
- Convergence to the optimal Q-function directly  
- Time complexity that scales with the product of number of states and actions  

---

### Question 5 (True/False)
Policy iteration always requires more iterations than value iteration to converge.

- True  
- False  

---

### Question 6 (Multiple Choice)
What is the primary difference between value iteration and Q iteration?

- Q iteration operates directly on the Q-function  
- Value iteration does not use Bellman updates  
- Q iteration avoids any dependence on transitions  
- Value iteration uses a greedy policy improvement step every round  

---

### Question 7 (True/False)
Policy iteration alternates between evaluating the current policy and greedily updating it.

- True  
- False  

---

### Question 8 (Multiple Select)
Why are dynamic programming approaches to solving MDPs often impractical for large environments?

- They assume continuous action spaces  
- They require summing over all states and actions  
- They donâ€™t converge for stochastic transitions  
- The number of states in environments like Atari or chess is extremely large  

---

### Question 9 (Multiple Choice)
What is the purpose of the policy improvement step in policy iteration?

- To sample new actions from a uniform distribution  
- To randomly explore actions not taken before  
- To choose the action that maximizes the expected value at each state  
- To normalize the transition matrix  

---

### Question 10 (True/False)
The recursive Bellman equations form the theoretical foundation for value iteration, Q iteration, and policy iteration.

- True  
- False  
