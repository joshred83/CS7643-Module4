# Answer Key: Policy Gradients and Actor-Critic Methods

---

### Question 1  
**Correct Answer:** True  
**Explanation:**  
Policy-based methods directly parameterize and optimize a policy.  
> "Policy based methods on the other hand directly parameterize a policy and optimize it to maximize returns."

---

### Question 2  
**Correct Answer:** Value-based methods optimize value functions, policy-based methods optimize policies directly  
**Explanation:**  
Value-based and policy-based methods differ in their optimization targets.  
> "Among the types of methods using RL value based methods learn q functions... Policy based methods... directly parameterize a policy..."

---

### Question 3  
**Correct Answers:** Sampling trajectories using the current policy, Applying the log-derivative trick to compute the gradient  
**Explanation:**  
REINFORCE involves sampling data and using the log-derivative trick to compute gradients.  
> "We simply need to collect a small batch of trajectories using the current pi theta..."  
> "...in order to apply a relation known as the log derivative trick..."

---

### Question 4  
**Correct Answer:** To simplify the integral of the gradient of a probability density  
**Explanation:**  
The log-derivative trick transforms the intractable gradient of an integral into an expectation.  
> "...we can write the expectation as an integral... and apply a relation known as the log derivative trick..."

---

### Question 5  
**Correct Answer:** True  
**Explanation:**  
Gradient updates push the probability of successful actions higher.  
> "...the gradient update will push the probability of the chosen actions to be either higher or lower depending on the sign and magnitude of the total reward..."

---

### Question 6  
**Correct Answers:** It suffers from high variance in gradient estimates, It does not assign credit to individual actions within a trajectory  
**Explanation:**  
REINFORCE suffers from variance and lacks fine-grained credit assignment.  
> "...we are left with a coarse level feedback for the entire sequence..."  
> "...New variants of this algorithm have been proposed that aim to reduce the variance..."

---

### Question 7  
**Correct Answer:** To reduce variance in gradient estimation  
**Explanation:**  
Baselines are introduced to reduce variance without changing the expected gradient.  
> "...subtracting some baseline... will preserve the mean... while possibly reducing the variance..."

---

### Question 8  
**Correct Answer:** False  
**Explanation:**  
Subtracting a baseline that does not depend on action preserves the mean.  
> "...subtracting some baseline... will preserve the mean of the gradient expectation..."

---

### Question 9  
**Correct Answer:** The advantage function (Q minus V)  
**Explanation:**  
Advantage Actor-Critic uses \( A = Q - V \) as the scaling factor.  
> "...advantage actor-critic that substitutes the reward with the advantage... defined as the Q function minus the V function."

---

### Question 10  
**Correct Answers:** A learned Q-function, Policy gradient updates, Advantage estimates, A reward baseline derived from the policyâ€™s V-function  
**Explanation:**  
Actor-Critic methods include all of these elements as discussed.  
> "...actor-critic algorithm that replaces rewards with the Q function..."  
> "...advantage actor-critic... defined as the Q function minus the V function."

