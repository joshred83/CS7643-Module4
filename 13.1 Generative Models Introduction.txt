[00:00:00]
>> In this lesson, we'll talk about a different class of unsupervised learning methods called generative modeling. Here we take a probabilistic view of unsupervised learning and try to estimate the probability distribution over the input space. Now, this is a hard task and so we'll make several assumptions and simplifications and we'll talk about the most popular methods in this lesson.

[00:00:22]
A large part of deep learning success has come from Supervised Learning. That is we have a large data set full of examples with answers, for each example, we know what the correct answer is. We would like to also support Unsupervised Learning, where we take only unlabeled data and understand something about it.

[00:00:40]
At the heart of it, we can view this as a probabilistic model, that is we wanna represent the joint distribution over the input dimension. This is a very intractable and hard thing to do. And so we'll have to make various assumptions or simplifications in order to make this feasible.

[00:00:57]
In Supervised Learning, we can categorize the different tasks we may want to do. For example, into bins such as classification and regression, based on the inputs and outputs. Similarly, in Unsupervised Learning, there's a whole host of different tasks we may want to do. In this lesson, we'll focus on Density Estimation, that is producing a model of the probability distribution over the input space.

[00:01:18]
Or in some cases, we may want to just have the ability to generate samples from this distribution, that is actually generate artificial examples that come from this distribution. This has a rich history in traditional unsupervised learning methods that existed before the deep learning resurgence. For example, Gaussian mixture models also produce some estimate of the probability distribution over the input space.

[00:01:42]
However, these methods have severe deficiencies when the input is very high dimensional. And since deep learning is very good at learning features that extract meaningful information in a low dimensional embedding from high dimensional data. The question becomes, can we apply similar ideas to neural network-based learning methods? It's important to understand the distinction between Discriminative and Generative models.

[00:02:07]
You may have learned about this in your machine learning class, discriminative models, model the conditional distribution probability of the label given the input. Examples of this include neural networks, support vector machines, random forests and so on. You may not have known that this is actually what we're doing, because really we just approximate this probability distribution using a blackbox neural network.

[00:02:31]
We haven't done any additional probabilistic reasoning. However, if you've taken a machine learning course, you may have learned that there's an entire probabilistic and statistical view of machine learning. Where we can actually write out these probability distributions and reason about them. For example, to derive what loss functions we may want to use.

[00:02:49]
Generative models, on the other hand, models the distribution over the input space. Now, this is a very complex distribution, and so there's a question of how we can actually model it. Just, like discriminative models we can have a parametric approximation of this distribution. That is we can have a set of parameterized models p(x,theta).

[00:03:09]
And use the principle of maximum likelihood to optimize the parameters given the unlabeled data set. Here p(x,theta) is our likelihood and we wanna maximize it or are real theta, that is we want our model to output high probabilities for the real deta. We can use this as the optimal set of parameters, data star, being the argmax data of the product of the likelihood of each example.

[00:03:36]
This is because the examples are drawn independently and identically, that is we're just sampling independently from this distribution. And so the likelihoods of all of the data can be decomposed as the product of the likelihood of each piece of data. We can then take the log of this because we're maximizing it, and this turns out into a sum of log likelihoods.

[00:03:57]
These are called generative models because they can often generate samples. Once, we have some parameters data, we can also generate examples from this distribution that we're approximating. Some examples include multi-variate Gaussian model, where we have mu and sigma, that is the mean and co-variants. And what we wanna do is estimate these values from the data, and then given these estimates, we can just sample from the multi-variate Gaussian to actually produce example.

[00:04:27]
Now, of course, this is very difficult, especially for high dimensional data. Because generative modeling is so difficult, there's been a whole host of different methods that have been developed. These can be categorized in various ways, for example, whether they use the maximum likelihood principle in order to optimize their parameters.

[00:04:44]
There are methods that perform explicit tractable density estimation. That is, they simplify the joint distribution into some factorized model consisting of simpler components and then learn parameters for those. There are methods that perform approximations of various kinds, that is, they learn distributions that approximate the true joint distribution.

[00:05:08]
And then there are implicit density models, where we don't actually model the density itself. Rather, we're just able to perform tasks such as sampling from the density. That is we can generate samples from the probability distribution, but we can't really have an explicit model for it that we learned.

[00:05:26]
We'll cover the three most popular methods across the spectrum, in order to give you a flavor of all the different methods that exist.

