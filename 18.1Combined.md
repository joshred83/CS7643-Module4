# Quiz: Introduction to Unsupervised, Semi-Supervised, and Few-Shot Learning



### Question 1 (True/False)  
In few-shot learning, it is common to assume access to a large amount of unlabeled data.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Few-shot learning typically assumes access to a small auxiliary labeled dataset, not unlabeled data.  
> "In few-shot learning... we have only 1 to 5 examples per category. In the vanilla setting, we also don't have any unlabeled data..."
</details>

---



### Question 2 (Multi-Select)  
Which of the following accurately describe key characteristics and assumptions of semi-supervised learning? (Select all that apply)

- It combines a small amount of labeled data with a large amount of unlabeled data
- It assumes that data points in the same cluster should have the same label
- It leverages the structure of unlabeled data to improve supervised learning
- It requires completely labeled datasets to function
- It often assumes that the decision boundary should lie in low-density regions
- It can use techniques like pseudo-labeling to iteratively expand the labeled set

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Combines labeled and unlabeled data, ✅ Assumes same-cluster points have same label, ✅ Leverages unlabeled data structure, ✅ Assumes decision boundaries in low-density regions, ✅ Can use pseudo-labeling  
**Explanation:**  
Semi-supervised learning incorporates several key assumptions and techniques.  
> "In semi-supervised learning... we also have a large set of unlabeled data..."
> "Semi-supervised methods leverage assumptions about data clustering and the decision boundary placement."
> "One approach is to iteratively expand the labeled set using high-confidence predictions as pseudo-labels."
</details>

---



### Question 3 (Multiple Select)  
Which of the following are traditional unsupervised learning tasks mentioned in the lecture?

- Density estimation  
- Clustering  
- Meta-learning  
- Supervised contrastive learning

<details>
<summary>Show Answer</summary>

**Correct Answers:** Density estimation, Clustering  
**Explanation:**  
These are listed as classic unsupervised tasks.  
> "...purely unsupervised learning where we can perform tasks, such as clustering or density estimation."
</details>

---



### Question 4 (Multi-Select)  
Which of the following accurately describe the motivations and properties of pseudo-labeling in semi-supervised learning? (Select all that apply)

- To generate labeled examples from confident predictions on unlabeled data
- To iteratively expand the training set with model-generated labels
- To leverage the structure of unlabeled data to improve generalization
- To implement a form of self-training that can strengthen decision boundaries
- To replace the need for any manually labeled data
- To allow models to benefit from the distribution information in unlabeled data

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Generate from confident predictions, ✅ Iteratively expand training set, ✅ Leverage unlabeled data structure, ✅ Implement self-training, ✅ Benefit from distribution information  
**Explanation:**  
Pseudo-labeling serves multiple important purposes in semi-supervised learning.  
> "We can then take that and convert it into a pseudo label... and put them into the training set and retrain."
> "This allows the model to iteratively improve by incorporating its own confident predictions into training."
> "The unlabeled data provides information about the input distribution that can help the model generalize better."
</details>

---



### Question 5 (True/False)  
Self-supervised learning uses human-annotated labels to guide feature learning.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Self-supervised learning does not use human-annotated labels but instead creates tasks from unlabeled data.  
> "In self-supervised learning... we assume we have no labels. All we have are unlabeled examples..."
</details>

---



### Question 6 (Multiple Select)  
What are some of the challenges in feature learning from unlabeled data?

- Selecting appropriate loss functions  
- Designing surrogate tasks that are neither too easy nor too hard  
- Minimizing overfitting with small datasets  
- Learning a metric with labeled anchors

<details>
<summary>Show Answer</summary>

**Correct Answers:** Selecting appropriate loss functions, Designing surrogate tasks that are neither too easy nor too hard  
**Explanation:**  
The lecture highlights challenges in selecting suitable losses and task difficulty.  
> "What loss functions should we use?"  
> "...surrogate tasks that hopefully aren't too easy... and aren't too hard."
</details>

---



### Question 7 (Multi-Select)  
Which of the following accurately describe "surrogate tasks" in self-supervised learning? (Select all that apply)

- They are designed to force networks to learn useful representations, though the tasks themselves may be artificial
- They can involve predicting properties of data that are automatically generated (like rotations)
- They create supervision signals from the data itself without human annotation
- They are often designed based on domain knowledge about what makes good features for downstream tasks
- They require human labels to define the learning objectives
- They can involve reconstructing part of the input from another part

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Designed to force learning useful representations, ✅ Can involve predicting generated properties, ✅ Create supervision without human annotation, ✅ Often designed with domain knowledge, ✅ Can involve reconstruction tasks  
**Explanation:**  
Surrogate tasks have several key characteristics in self-supervised learning.  
> "...we'll rotate an image... and try to make the neural network predict which rotations we applied... we actually don't care about it... but we're trying to force the neural network to learn..."
> "These tasks generate their own supervision signal from the data structure itself."
> "Examples include predicting image rotations, solving jigsaw puzzles from image patches, and inpainting missing regions."
</details>

---



### Question 8 (True/False)  
Metric learning is a form of unsupervised learning that assumes human-annotated pairwise similarities.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Metric learning can be done without labeled pairwise supervision by learning similarity functions from data.  
> "Or there are methods to do what's called metric learning where you try to learn a distance metric as well."
</details>

---



### Question 9 (Multiple Select)  
Which of the following strategies are used in semi-supervised learning as described in the lecture?

- Using strong and weak augmentations  
- Cross-entropy loss with hard pseudo labels  
- Training exclusively on confidently labeled examples  
- Using data to self-generate soft targets for knowledge distillation

<details>
<summary>Show Answer</summary>

**Correct Answers:** Using strong and weak augmentations, Cross-entropy loss with hard pseudo labels, Using data to self-generate soft targets for knowledge distillation  
**Explanation:**  
These techniques are all discussed as part of the semi-supervised learning pipeline.  
> "...perform augmentation of two types... weak form... strong augmentation..."  
> "We can take these confident pseudo labels... and retrain."  
> "...use what we've learned... in the form of knowledge distillation..."
</details>

---



### Question 10 (Multi-Select)  
Which of the following accurately describe the motivations and approaches of meta-learning in few-shot scenarios? (Select all that apply)

- To learn an initialization that adapts well to new tasks with few training steps
- To simulate the test-time scenario during meta-training
- To leverage experience across multiple tasks to improve generalization to new tasks
- To maximize entropy of predictions on test data
- To develop models that can quickly adapt to new information with minimal fine-tuning
- To eliminate the need for task-specific finetuning altogether

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Learn an initialization for quick adaptation, ✅ Simulate test-time scenario during training, ✅ Leverage experience across tasks, ✅ Develop models for quick adaptation  
**Explanation:**  
Meta-learning approaches have several important goals in few-shot learning.  
> "...learning an initialization that after you go through such process will be effective..."
> "The goal is to learn from many related tasks during meta-training so that the model can quickly adapt to new tasks at test time."
> "This way, we're simulating the few-shot scenario that will be encountered at test time."
</details>