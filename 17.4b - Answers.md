# Answer Key: Deep Q-Learning

---

### Question 1  
**Correct Answer:** False  
**Explanation:**  
Deep Q-Learning avoids full for-loops and instead uses function approximation and minibatches.  
> "Instead of having a for loop over all states to update the Q-network, as was done in Q-iteration, we introduced a regression objective..."

---

### Question 2  
**Correct Answer:** It provides a recursive objective for value prediction  
**Explanation:**  
The Bellman equation is used to generate Q-value targets recursively.  
> "The update for our Q-network will again be inspired, by the recursive bellman optimality equation."

---

### Question 3  
**Correct Answers:** Gradient descent updates, A fixed replay buffer for data sampling, A target Q-network updated periodically  
**Explanation:**  
Training involves minibatch gradient descent, a replay buffer, and two networks (Qnew and Qold).  
> "We introduced a regression objective..."  
> "Two copies of the Q-network are maintained... Qold and Qnew..."  
> "Deep Q-learning employs an experience replay buffer..."

---

### Question 4  
**Correct Answer:** To stabilize training by decoupling target predictions  
**Explanation:**  
Using a separate Qold network helps reduce instability in loss updates.  
> "Using a single Q-network, makes the loss minimization unstable... Instead, two copies... are maintained..."

---

### Question 5  
**Correct Answer:** False  
**Explanation:**  
Two Q-networks are maintained: one for predictions (Qnew) and one as a fixed target (Qold).  
> "Qnew parameters are updated while preventing any update to the Qold parameters."

---

### Question 6  
**Correct Answers:** It can prevent exploration of better strategies, It causes training data to be highly correlated  
**Explanation:**  
Using a greedy data collection policy limits exploration and causes data bias.  
> "...it will not have incentive to explore other less rewarding states..."  
> "The data... will be highly correlated with similar states, actions and rewards."

---

### Question 7  
**Correct Answer:** It balances exploration and exploitation during data gathering  
**Explanation:**  
Epsilon-greedy selects random actions occasionally to encourage exploration.  
> "...a random action is chosen with a typically small epsilon probability, and the greedy action is selected otherwise."

---

### Question 8  
**Correct Answer:** False  
**Explanation:**  
Replay buffers store a range of past experiences, not just recent ones, to reduce correlation.  
> "The buffer is a finite size and older samples are discarded in favor of newer ones... to lower the correlation..."

---

### Question 9  
**Correct Answers:** Using two separate Q-networks, Sampling random minibatches from past experiences  
**Explanation**
