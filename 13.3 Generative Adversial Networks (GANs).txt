[00:00:00]
>> In this lesson we'll talk about a different class of generative models called generative adversarial networks. Generative adversarial networks or GANS did not learn an explicit density function p of x, rather they fit under the implicit density category. What that means is rather than learn a parameterised density function We will instead learn a parameterised generation process that can give us samples from the joint distribution.

[00:00:28]
We won't have an explicit density though, to perform things like classification or things like that. But we'll still be able to use the unlabeled data to learn how to generate new samples. That fits that distribution of the unlabeled data, or even learn features that might be good for downstream tasks.

[00:00:48]
Implicit generative models do not actually learn an explicit model for p of x. Instead, we learn to generate or sample from p of x. This is useful for many different applications. For example, it turns out we can potentially learn good feature representations from unlabeled data alone. We can also perform data augmentation and so we can generate new images or example.

[00:01:12]
We can also Learn a simulator for the dynamics of the world that is we can roll out potential features. And this allows you to train reinforcement learning without actually having to experience the world. And in order to do this, we have to do several different things. We have to learn how to sample from a neural network output.

[00:01:32]
It's not clear how to produce random outputs. Given some fixed input thus far, we've only dealt with a particular input and then generate a particular output. We also will use the notion of adversarial training. We'll use one that works predictions to train another networks, lost function. This is essentially can be seen as a dynamic loss function and you have two dural networks learning at the same time that we pit against each other.

[00:01:59]
There's also lots of tricks to make this more stable. Because you have a game theoretic optimization pitting two neural networks against each other, the complex dynamics of learning will actually turn out to be quite difficult to train. And so there's a lot of different tricks that researchers have developed in order to make this more stable and effective.

[00:02:19]
We would like to sample from a very complex distribution p of x using a neural network. We're going to use a simple idea of first sampling from a simple distribution, they a Gaussian, where we know how to generate samples from, and then we'll actually learn a transformation function that transforms samples from that simple distribution to samples from the complex distribution.

[00:02:43]
Here's an illustration. We'll take a particular parameterize distribution that we know how to sample from, for example, a Gaussian, and then we'll sample many times from that distribution. For each sample, we'll feed it through a neural network that will generate some other sample. And then we'll essentially over many samples approximate this distribution p of x, Now this transformation function has to be pretty complex, but that's okay.

[00:03:10]
We have deep learning where we can learn really complex functions as a result. Here's a concrete instantiation of this idea. We can sample from a Gaussian many different times to generate a vector of random numbers that are independently generated. We can then take in this vector and use a generator that is a network that takes a vector and up samples into an image or transforms it into an image through a complex set of convolution and spooling layers, in order to output a complete image.

[00:03:43]
Unlike the tractable density models pixel CNN. For pixel. This is done in one shot, we're just taking a vector of random numbers and feeding it through one convolutional neural network and outputting an image. This is interesting. It allows us to automatically generate images. Now we initialize this from scratch, and so the initial images that it generates.

[00:04:07]
So thoughts, what do you think? I mean, in real towns, you know, just this, the difference between the social engineering military and the participating logic, do you find yourself more aligned to us? The thought of spiffy logic as social motivation thinkers or the social engineering? The key idea of generative adversarial networks is to have another network that distinguishes between real and generated or fake images.

[00:04:33]
And the reason we wanna do this is that how well the discriminator performs. It can be used as a signal to determine how well the generator is doing. One interesting thing is that both of these neural networks are learning at the same time. So in the beginning, the discriminator is not that good.

[00:04:52]
However, once we train the discriminator to perform better and better discrimination between real and fake images, The generator will have to get better. Otherwise, if the generator continues generating random noise images, the discriminator will do really really well. And that can be used as a signal that is a loss function in order to update the generator that is update the generator parameters so that what it generates reduces the loss given by the discriminator.

[00:05:21]
What's interesting here is that you're pitting the two neural networks against each other. This can also be seen as just an adversarial loss function. Rather than have a fixed ground truth that we train the generator against. We're going to just have another neural network that's learning at the same time provide the signal that comes from a loss function in order to update the generator.

[00:05:42]
And it hopeful ideal scenario is that both of these continue to get better and better and better, such that the generator continues to improve and generate more and more realistic images, while the discriminator is able to get better and better and more finally discriminate between real and fake images.

[00:06:00]
The way we set this up is as follows. Again, everything we do in deep learning will be mini batch gradient descent. And so using the generator network will feed in a number of different random vectors, each of which will result in particular images that are generated. In the meantime, we'll also feed in real images.

[00:06:19]
So the mini-batch, for example, can have half real images and have fake images. This is fed to the discriminator, which is trying to determine whether something is real or fake. The key thing is that we know the answer. This is kind of like self supervision. We fed it real images or fake images, and we know which one we fed it so we know the ground truth answer.

[00:06:40]
So what we're going to want to do is update the weights for the generator to improve the realism of the generated image and update the weights of the discriminator to make it better and better at discriminating real fast From fake the key question is what the loss function should look like and note that here.

[00:06:59]
We have two optimizations happening simultaneously and so we'll have 2 different loss function or 1 loss function where one network is minimizing it while the other network is maximizing it. Since we have two networks competing, this is a minimax two player game. And so it has really rich ties to game theory, although it's not clear that we can analyze this includes form, for example, figuring out whether it has a Nash equilibria because we're using very deep complex neural networks that are non convex.

[00:07:32]
Here's the full mini max objective. You notice on the left side, we have a min of the G generator and the max of the D the discriminator. And so we're taking the same objective function, but one network is optimizing its parameters to minimize it while the other is optimizing its own parameters, which are different.

[00:07:52]
To maximize it, the way it works is on the right hand side, we have a sampling from the fake data that is Z comes from PZ of Z, which is just our prior distribution. That is a Gaussian. To the sample a vector of. Normally distributed random numbers, and then we have log of one minus d of g of z.

[00:08:12]
Here, the discriminator is supposed to output a probability of one for the real image and zero for the fake image. And so if d of g of z should be zero, because it's a fake image, then we want one minus d of g of z. And then we take the log of that.

[00:08:30]
And so this is essentially saying how well does this discriminator going to do? And the generator wants to minimize this because the generator wants to fool the discriminator. So again, this right side of the objective function is saying how well is the discriminator doing on fake data and generator wants to minimize that because it wants the discriminator to not do well.

[00:08:52]
Note that the objective function for the generator only is affected by the right side. That is the gradients for the first part of the term of the objective function is zero. Because if you notice it only has log of D of x and there's no term there that depends on G.

[00:09:08]
So no amount of changing G parameters will affect that part of the objective. On the other hand, the discriminator is doing the opposite. We can sample from the fake data and the discriminator wants to maximize this, how well the discriminator does, that is, it wants to output zero for the fake data.

[00:09:27]
At the same time we sample a mini-max from the real and again the discriminator is maximizing this Note that for the generator, only one part of this objective function is valid. On the left side of the objective, we just have [log of D(x)]. That is the discriminator output on real data.

[00:09:45]
And nothing we change about the generator's parameters effects this part of the objective That is a gradient zero. So really the gradient for the generator only comes from the right side of the term, whereas the gradients for the discriminator comes from both. It wants to both do well on the real as well as on the fake in discriminating them.

[00:10:04]
Let's take again each discriminator or generators perspectives in order to go through this once more. This is the objective function. And again remember a D(x), where x is real image is the discriminators output probability where we want it to be 1 If it's real, so in this case, we do want it to be 1 because x is real, g of z is a generated image.

[00:10:26]
So we want D of g of z to be 0 if you're the discriminator. So the discriminator wants to maximize. Is this, it wants to put D of X up to one because it is a real image. X is a real image. And on the other hand, they wants to push also one minus D of Z to one, because then you're pushing D of Goz down to zero, meaning that the discriminator says zero for the fake image.

[00:10:54]
Which is what the discriminator wants to. So in other words, the discriminator wants to classify real images as real. That is output a probability one and fake images as fake, that is a probability of zero. And so we'll use both of these terms in the objective function and derive the backprop gradients in order to update the parameters of the discriminator so that it does a better job.

[00:11:16]
Of discriminating from the generator's perspective, we do the opposite, except that one part of the term doesn't really have a G in it. So it doesn't really matter. But the right side of the objective is one minus D O G of Z, which we want to push down to zero.

[00:11:31]
So that D(G(z)) is pushed up to 1. That means that it taking fake data and giving it a probability of being real as 1, which is what the generator wants because it wants to fool the discriminator. This means that the generator is fooling the discriminator that is at succeeding at generating images that the discriminator can't discriminate from real.

[00:11:52]
So again, we'll update the generator using the gradients from this objective. So we're tuning both the generation process and the feature extraction in order to essentially output more and more realistic images. Here is a visual depiction of the same thing. Again, we'll have a mini batch of real and fake data, we'll have the generator producing the fake data, where we start from a vector of random numbers, where each random number in the vector is sampled from a normal distribution.

[00:12:24]
And then we'll also have in the mini batch real images. And we'll feed those discriminator, and each part will have an objective, the generators objective only touches the one minus D of G of Z, because that's the only part of the objective that has G in it. And we're going to average this over the mini batch, that is over the fake data in the mini batch.

[00:12:46]
The discriminator loss on the other hand, includes both of the terms it wants to both make sure that it outputs a high probability for the real data and a low probability for the fake data. And so we'll perform backprop and average the gradients over the entire mini batch.

[00:13:01]
And it turns out that the generators part of the objective is actually not very good in terms of its gradient property. is is another example where plotting the objective function and reasoning about where the gradients are high or low will enable us to determine whether there's something wrong.

[00:13:19]
If we plot the log of one minus D of G of Z, we get the blue curve on the X axis. You get DFD of Z. That is the discriminators probability on the generated images. When this is high high or close to one, that means that it thinks that the generated images are real.

[00:13:38]
That is it's being fooled. In this case, you could see in that region, the gradients are pretty high. When the discriminator gives the generated or fake images, a low probability that is it's not being fooled, it's doing a good job. That's where the gradients are pretty low. If you think about it, this is actually the opposite of what we want in terms of gradients when we're using it to update the generator.

[00:14:00]
If the discriminator is giving the fake images a higher probability of being real, that means the generated images are already good. So we don't need high gradient values. On the other hand, when the discriminator is giving the fake images of probability close to zero, that means it's not being fooled.

[00:14:16]
So we need to have high gradient values to update the generator so that it produces more realistic images. So we can have an alternative objective. Rather than have a min max, we can have a Max Max where the generators objective is actually slightly different, but it's also maximizing it.

[00:14:34]
In this case, we're going to maximize. Is the log of D of G of Z you could see on the red curve that we have better gradient. That is when the discriminator has a low probability. That is it's not being fooled. Then we have high gradients and we can use that to improve the generator in order to produce more and more realistic images.

[00:14:53]
Here's the full algorithm. We're going to use mini batch stochastic gradient descend. There's a number of hyper parameters here. One thing I didn't talk about is how you actually optimize. We can alternate the optimization of the generator and the discriminator. And actually, there's some theory that shows that if you optimize the discriminator for more than one step, you might do better.

[00:15:15]
And so here there's a hyper parameter k that tells us how many steps do we update The discriminator so we have this standard training loop where we have a number of iterations and here for K steps will update the discriminator will sample a mini batch m of noise samples, that is the Z.

[00:15:34]
And then we'll sample a mini batch of M actual real examples from the training set. And then we'll also feed the noise samples into the generator, and then we'll update the discriminator by ascending. That is maximizing the objective using stochastic gradient. That is we'll average over the minibatch the gradient tier of that objective function.

[00:15:56]
Once we've updated the discriminator, we'll do the same for the generator. We'll sample another mini batch of noise samples. And then, we'll update the generator by descending it's stochastic gradient, or as we mentioned, we can ascend the slightly modified objective, and then we can just continue to iterate.

[00:16:16]
At the end of the training, we'll have an implicit generator model that is, we can generate samples from the training distribution of unlabeled data. We can take random noise vectors, feed it through the generator and produce images. We actually don't need the discriminator for this process, we can just throw it away, or if we want to use these processes for unsupervised learning of features, we can actually take the features from the discriminator and use it for whatever downstream task we want.

[00:16:47]
Here is an example of a few sets of image data sets that you can train the generator on. What you can see here is these initial examples which were little bit during the early days of generative adversarial networks are pretty low resolution, they kind of generate pretty realistic hand digits.

[00:17:08]
Though faces and more complex scenes are a little bit more difficult and the resolutions are pretty small still. One of the reasons that GANs weren't as good in the early days is that training these mini-max objective functions are just intrinsically hard. You have all sorts of strange dynamics that can occur between the generator and discriminator.

[00:17:28]
So a number of advancements have been developed, including more stable architectures, regularization methods to improve the optimization, and things like progressive growing, that is learning a generative model for small images and smaller networks. And then slowly scaling those up and reusing the learning from the smaller networks to the larger ones, or just scaling these two very large hyperparameter models.

[00:17:54]
One of the early innovations that occurred in GANs is just tuning the architectures, it turns out that different architectures are more stable than others for training this objective. This is from a paper called DC GANs where several findings were developed. For example, replacing pooling layers with strided convolution, using batchnorm both in the generator and discriminator, removing the fully connected hidden layers for deeper architectures.

[00:18:22]
Using ReLUs except for certain places such as the output, or using LeakyReLU activations for the discriminator. Again, it's not clear why these things in particular help but through empirical investigation, researchers found it to be a good solution for stable GAN training. Again, training GANs is difficult due to the mini-max objective.

[00:18:43]
You can actually have all sorts of strange dynamics of the training, for example, what if the generator just learns to memorize and output samples of your training? This actually will do really well, it'll fool the discriminator because it can't tell those real samples from other real samples. The problem is you don't have a lot of diversity or variety, you're not truly sampling from the distribution, you're just outputting the samples that you already have.

[00:19:09]
Another problem is only generating parts of the distribution, so you can do really well, for example, on male faces or only on female faces, but not both. There's something called mode collapse where you can't capture all the modes of the distribution. There are several regularization methods that were developed and these are typically theoretically motivated.

[00:19:31]
I'm not going to go through the entire library of regularization method since there are dozens at this point, but some really simple ones, for example, just add noise to the real images. It turns out that this can actually be theoretically-motivated and derived, as researchers improved all of these aspects that is, architectures, objectives, and optimization and regularization methods, as well as scaling.

[00:19:55]
Researchers began to be able to generate extremely realistic images. In fact, these images that you see here are not real they're fake, and you can actually see that it's not that easy to tell them apart from real images, there are some details that are often wrong. For example, eyes are hard, especially making both eyes be consistent with a gate or in appearance, however, overall, these actually have pretty good details, which is quite surprising.

[00:20:23]
Just like language models however, they're not without failures, you can have, for example, different types of class confusion that occurs where the generator confuses different types of classes together, such as a tennis ball or in some animal. These are kind of comical failures but what's interesting is that they're not that bad, we can understand that this is a failure mode that's rather reasonable.

[00:20:46]
It turns out we can also generate videos that are consistent across time, this shows those are really interesting use case, you should check out the YouTube video link here. What this is showing is images from videos that are completely generated. What the researchers did is take videos from professional dancers and extracted their body pose.

[00:21:04]
You can then take as input a 10 second footage of you moving around randomly, and then the GAN is able to automatically generate an entire video of you dancing like the professional dancer. This is quite cool and has some implications for things like bias and misinformation but check out the video, I think you'll like it.

[00:21:24]
In summary, generative adversarial networks can produce amazing images and videos and these days other adaptations have been developed to produce audio waveforms and so on. They still have some drawbacks, high-fidelity generation is pretty heavy to train and requires a lot of compute, training can be unstable, so a lot of x experience and research is needed to make it stable.

[00:21:48]
At the end, we don't actually have an explicit model for the distribution P(x), and so if you wanna do things like classification, it's not clear how to directly take what's learned, really, we just take the features and then fine-tune them. There's a large number of extensions ranging from GANs conditioned on labelled data or other information if it's available.

[00:22:07]
And the notion of adversarial losses, for example, a discriminator that discriminates between real or fake, or one data set to another data set are actually useful in a number of different domains and have been used successfully.

