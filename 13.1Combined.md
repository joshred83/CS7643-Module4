# Quiz: Generative Models - Deep Dive



### Question 1 (Multi-Select)

Which of the following accurately describe the role and goals of generative models in unsupervised learning?

- [ ] A. They only classify input data into known categories.

- [ ] B. They estimate the probability distribution over the input space.

- [ ] C. They can be used to generate new, artificial samples resembling the data.

- [ ] D. They are used for tasks such as density estimation.

- [ ] E. They require labeled data to function effectively.

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Generative models do **not require labeled data** and **do more than classification**.

> "Here we take a probabilistic view of unsupervised learning and try to estimate the probability distribution over the input space."  
> "In this lesson, we'll focus on Density Estimation... we may want to just have the ability to generate samples from this distribution..."
</details>

### Question 2 (Multi-Select)

Which of the following traditional methods were used for density estimation or similar probabilistic modeling before the resurgence of deep learning? (Select all that apply)

- [ ] A. Naive Bayes classifiers

- [ ] B. Support Vector Machines

- [ ] C. Gaussian Mixture Models

- [ ] D. Linear Discriminant Analysis

- [ ] E. Hidden Markov Models

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
GMMs were traditionally used for density estimation, though they struggle with high-dimensional input spaces. Naive Bayes and HMMs are also probabilistic models used for modeling data distributions.

> "For example, Gaussian mixture models also produce some estimate of the probability distribution over the input space. However, these methods have severe deficiencies when the input is very high dimensional."
</details>

### Question 3 (True/False)

Generative models model the conditional probability of labels given the input data.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Discriminative models model \( P(y|x) \); generative models model \( P(x) \).

> "Discriminative models, model the conditional distribution probability of the label given the input... Generative models, on the other hand, model the distribution over the input space."
</details>

### Question 4 (Multi-Select)

What are key challenges associated with modeling the joint distribution of high-dimensional data in generative models?

- [ ] A. High computational complexity

- [ ] B. Intractability of the exact probability distribution

- [ ] C. Lack of any theoretical basis

- [ ] D. Requirement of assumptions or simplifications

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Modeling \( P(x) \) directly is challenging without simplifying assumptions.

> "This is a very intractable and hard thing to do. And so we'll have to make various assumptions or simplifications in order to make this feasible."
</details>

### Question 5 (Multi-Select)

You are asked to design a generative model for a new high-dimensional image dataset. What strategies from the transcript are likely to help in tackling the complexity? (Select all that apply)

- [ ] A. Use deep learning architectures that reduce dimensionality

- [ ] B. Use rule-based symbolic reasoning

- [ ] C. Leverage embeddings to find low-dimensional representations

- [ ] D. Avoid simplifications to maintain full generality

- [ ] E. Apply parametric approximations of probability distributions

- [ ] F. Utilize maximum likelihood estimation principles

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
The transcript emphasizes several strategies for handling high-dimensional data: using deep architectures for dimensionality reduction, leveraging embeddings, applying parametric approximations, and using maximum likelihood estimation.

> "Deep learning is very good at learning features that extract meaningful information in a low dimensional embedding from high dimensional data."
> "Just, like discriminative models we can have a parametric approximation of this distribution."
> "And use the principle of maximum likelihood to optimize the parameters given the unlabeled data set."
</details>

### Question 6 (Multiple Choice)

Which of the following statements best captures the distinction between discriminative and generative models?

- [ ] A. Discriminative models generate new samples from the data distribution.

- [ ] B. Generative models are always more accurate than discriminative ones.

- [ ] C. Discriminative models estimate \( P(y|x) \), while generative models estimate \( P(x) \).

- [ ] D. Generative models require supervised data, while discriminative models do not.

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
This is a central distinction made in the lesson.

> "Discriminative models, model the conditional distribution probability of the label given the input... Generative models... model the distribution over the input space."
</details>

### Question 7 (Multi-Select)

According to the transcript, why is deep learning suitable for generative modeling of high-dimensional data?

- [ ] A. Deep learning methods can extract meaningful low-dimensional features.

- [ ] B. Neural networks can model complex distributions with sufficient flexibility.

- [ ] C. They inherently encode explicit probabilistic rules.

- [ ] D. They eliminate the need for any data preprocessing.

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Deep learning reduces dimensionality and provides flexibility in modeling.

> "Deep learning is very good at learning features that extract meaningful information in a low dimensional embedding..."  
> "Just, like discriminative models we can have a parametric approximation of this distribution."
</details>

### Question 8 (True/False)

The ability to generate new data samples is a core feature of generative models.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Generating samples is one of the core applications.

> "We may want to just have the ability to generate samples from this distribution, that is actually generate artificial examples..."
</details>

### Question 9 (Multi-Select)

Which principles or techniques are commonly used in optimizing and evaluating generative models? (Select all that apply)

- [ ] A. Cross-entropy minimization

- [ ] B. Maximum likelihood estimation

- [ ] C. Mean squared error

- [ ] D. KL divergence minimization

- [ ] E. Independent and identically distributed (i.i.d.) sampling assumption

- [ ] F. Log-likelihood summation

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Multiple optimization principles are used in generative modeling. MLE is the primary one, but KL divergence is also used for comparing distributions. The i.i.d. assumption and log-likelihood summation are key components of the optimization process.

> "We can have a set of parameterized models \( p(x, \theta) \)... use the principle of maximum likelihood"
> "This is because the examples are drawn independently and identically, that is we're just sampling independently from this distribution."
> "We can then take the log of this because we're maximizing it, and this turns out into a sum of log likelihoods."
</details>