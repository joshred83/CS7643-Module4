# Answer Key: Semi-Supervised Learning

---

### Question 1  
**Correct Answer:** False  
**Explanation:**  
Semi-Supervised Learning assumes a small amount of labeled data and a larger amount of unlabeled data.  
> "In Semi-Supervised Learning, we have a smaller amount of labelled data and a much larger amount of unlabeled data."

---

### Question 2  
**Correct Answer:** To match supervised learning performance with fewer labels by leveraging unlabeled data  
**Explanation:**  
The goal is to overcome limited labeled data by using abundant unlabeled data.  
> "Can we overcome the small amount of labeled data that we have using the larger amount of unlabeled data?"

---

### Question 3  
**Correct Answers:** Strong and weak data augmentations, Use of pseudo-labels derived from confident predictions, Cross-entropy loss applied to strongly augmented examples  
**Explanation:**  
FixMatch uses weak and strong augmentations, pseudo-labels, and trains with cross-entropy on strongly augmented data.  
> "We'll convert it into a one-hot vector... use those as the answer or the ground truth to train the bottom branch..."  
> "...two types of augmentation, a weak type and a strong type..."  
> "...cross-entropy, where we use the pseudo-label as the quote unquote ground truth..."

---

### Question 4  
**Correct Answer:** It filters out incorrect labels to maintain training quality  
**Explanation:**  
Confidence thresholds reduce noise in pseudo-labels to preserve training quality.  
> "The confidence threshold for pseudo-labeling is also very important... If this threshold is incorrect, then the pseudo-labels will be very noisy."

---

### Question 5  
**Correct Answer:** False  
**Explanation:**  
FixMatch is trained end-to-end, rather than in multiple distinct stages.  
> "The beauty of deep-learning is that we don't actually have to do this in multiple stages. We can have this entire pipeline trained in an end to end manner."

---

### Question 6  
**Correct Answers:** Cosine learning rate scheduler, Exponential moving average of weights, Large batch sizes for unlabeled data  
**Explanation:**  
All of these are mentioned as essential for FixMatch effectiveness.  
> "Other tricks, such as specialized learning rate, such as cosine learning rate scheduler, were also key..."  
> "Inference is actually done with exponential moving average of weights..."  
> "...large, unlabeled batch sizes are actually required for this algorithm to perform very well."

---

### Question 7  
**Correct Answer:** It may prevent many unlabeled examples from being used  
**Explanation:**  
A high threshold limits the inclusion of unlabeled examples, weakening the learning process.  
> "If this threshold is too strict, then we won't have many pseudo-labels that are added. And so essentially, you're not doing much with the unlabeled data."

---

### Question 8  
**Correct Answer:** True  
**Explanation:**  
Label Propagation relies on the assumption that nearby data points in feature space share labels.  
> "We want the unlabeled examples to be labeled similarly to the labeled examples based on how close the features are to such labeled examples."

---

### Question 9  
**Correct Answers:** Need for large unlabeled datasets, Difficulty in tuning hyperparameters, Poor generalization to real-world noisy datasets  
**Explanation:**  
The lecture highlights the need for scale, tuning challenges, and real-world generalization limits.  
> "We can actually get some datasets with hundreds of millions or billions of examples."  
> "There's always tuning and hyper-parameters that are required..."  
> "If you apply these methods to real-world datasets... then these methods... may not perform as well..."

---

### Question 10  
**Correct Answer:** That simple strategies like augmentation and thresholding can outperform more complex methods  
**Explanation:**  
FixMatch's simplicity is emphasized despite its strong performance.  
> "What's really nice is that the methods have actually gotten simpler... now all they use are very simple ideas, such as data augmentation and pseudo-labeling."
