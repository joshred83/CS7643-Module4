# Quiz: Unsupervised and Self-Supervised Learning



### Question 1 (True/False)  
Unsupervised learning requires labels to compute the loss functions used for training.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Unsupervised learning does not rely on labels; it uses patterns in the data.  
> "We assume we have no labels, all we have are unlabeled examples..."
</details>

---



### Question 2 (Multiple Choice)  
What is the primary purpose of autoencoders in unsupervised learning?

- To classify input data using pseudo-labels  
- To reconstruct labeled outputs from input data  
- To extract low-dimensional representations that capture important features  
- To optimize for high-resolution image synthesis

<details>
<summary>Show Answer</summary>

**Correct Answer:** To extract low-dimensional representations that capture important features  
**Explanation:**  
Autoencoders compress data into lower-dimensional representations for reconstruction.  
> "...we're trying to compress the image or any other data through some bottleneck..."
</details>

---



### Question 3 (Multiple Select)  
Which of the following are traditional tasks associated with unsupervised learning?

- Clustering  
- Dimensionality reduction  
- Reinforcement signal propagation  
- Density estimation

<details>
<summary>Show Answer</summary>

**Correct Answers:** Clustering, Dimensionality reduction, Density estimation  
**Explanation:**  
These are core classical tasks under unsupervised learning.  
> "...tasks such as clustering or density estimation..."  
> "...use an autoencoder to perform dimensionality reduction..."
</details>

---



### Question 4 (Multiple Choice)  
What does the “bottleneck” in an autoencoder architecture encourage?

- Overfitting to high-dimensional features  
- Learning trivial identity functions  
- Compression of inputs into meaningful low-dimensional embeddings  
- Increased accuracy through deep convolutional filters

<details>
<summary>Show Answer</summary>

**Correct Answer:** Compression of inputs into meaningful low-dimensional embeddings  
**Explanation:**  
The bottleneck enforces feature learning through constrained representation.  
> "...compress the image... and then decompress it again..."
</details>

---



### Question 5 (True/False)  
Autoencoders can be trained without any labels, using only the input data.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** True  
**Explanation:**  
Autoencoders learn from reconstruction loss, not labels.  
> "...we train the autoencoder to reconstruct the original image..."
</details>

---



### Question 6 (Multiple Select)  
What are characteristics of an effective feature space learned via unsupervised learning?

- High density clusters for semantically similar inputs  
- Widely dispersed examples regardless of semantic meaning  
- Low-density regions separating conceptually distinct inputs  
- Spatial locality preserved through convolutional filters

<details>
<summary>Show Answer</summary>

**Correct Answers:** High density clusters for semantically similar inputs, Low-density regions separating conceptually distinct inputs  
**Explanation:**  
Effective representations cluster similar inputs and separate dissimilar ones.  
> "...samples that are semantically similar are close together..."  
> "...semantically dissimilar examples are far apart..."
</details>

---



### Question 7 (Multiple Choice)  
In clustering-based unsupervised learning pipelines, what are pseudo-labels?

- Randomly assigned labels for evaluation  
- Labels assigned by a pretrained supervised classifier  
- Cluster assignments used as training targets for backpropagation  
- Binary indicators for supervised similarity

<details>
<summary>Show Answer</summary>

**Correct Answer:** Cluster assignments used as training targets for backpropagation  
**Explanation:**  
Clustering assigns pseudo-labels which are then used as targets.  
> "...use clustering algorithms to generate pseudo-labels for the unlabeled data..."
</details>

---



### Question 8 (Multiple Select)  
Which are potential issues when clustering is used to generate pseudo-labels?

- Empty clusters  
- Trivial solutions where all items are grouped into one cluster  
- Perfectly balanced partitions that prevent overfitting  
- Inability to use backpropagation through clustering

<details>
<summary>Show Answer</summary>

**Correct Answers:** Empty clusters, Trivial solutions where all items are grouped into one cluster  
**Explanation:**  
These are known risks in clustering-based training.  
> "...there may be empty clusters or trivial solutions where all data collapse to one cluster..."
</details>

---



### Question 9 (True/False)  
Surrogate tasks are designed to train models using freely available labels from data transformations.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** True  
**Explanation:**  
Surrogate tasks create labels from data transformations.  
> "...a surrogate task... where we try to predict the rotation... even though the original image has no label."
</details>

---



### Question 10 (Multiple Choice)  
Why is predicting image rotation considered a useful surrogate task?

- It trains the network to detect object boundaries precisely  
- It forces the model to recognize semantic content of images  
- It directly improves classification performance on downstream tasks  
- It encodes time-based sequence relationships

<details>
<summary>Show Answer</summary>

**Correct Answer:** It forces the model to recognize semantic content of images  
**Explanation:**  
Rotation prediction forces semantic awareness.  
> "...predict which of the four rotations... the neural network must understand whether there is an upright dog..."
</details>

---



### Question 11 (Multiple Select)  
Which of the following are common surrogate tasks for self-supervised learning?

- Colorization  
- Jigsaw puzzle solving  
- Rotation prediction  
- GAN-based style transfer

<details>
<summary>Show Answer</summary>

**Correct Answers:** Colorization, Jigsaw puzzle solving, Rotation prediction  
**Explanation:**  
These are classic self-supervised tasks.  
> "...predict the correct color... jigsaw puzzles... rotation prediction..."
</details>

---



### Question 12 (True/False)  
The quality of learned representations is typically evaluated by training a complex nonlinear classifier on top of the encoder.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Representations are evaluated using a simple linear classifier to avoid confounding effects.  
> "We will train a linear classifier on top of the features... and test the accuracy..."
</details>

---



### Question 13 (Multiple Choice)  
What is the goal of instance discrimination in contrastive learning?

- Maximize diversity of the input images  
- Align representations of augmentations of the same image while separating others  
- Match latent codes across different modalities  
- Use nearest neighbors for label propagation

<details>
<summary>Show Answer</summary>

**Correct Answer:** Align representations of augmentations of the same image while separating others  
**Explanation:**  
Instance discrimination aims to align augmentations and separate distinct inputs.  
> "...make the representations of the augmentations of the same image close together... other examples farther apart..."
</details>

---



### Question 14 (Multiple Select)  
Which techniques improve the efficiency of instance discrimination training?

- Using mini-batch negatives  
- Maintaining a memory bank of encoded features  
- Sampling only hard negatives manually  
- Using an exponential moving average to stabilize key encoders

<details>
<summary>Show Answer</summary>

**Correct Answers:** Using mini-batch negatives, Maintaining a memory bank of encoded features, Using an exponential moving average to stabilize key encoders  
**Explanation:**  
All of these are techniques used to improve training in contrastive learning.  
> "...use mini-batch negatives... use a memory bank... exponential moving average of the encoder weights..."
</details>

---



### Question 15 (Multiple Choice)  
What is a key advantage of self-supervised representations learned via contrastive learning?

- They generalize poorly but train quickly  
- They replace labeled data with GAN-generated features  
- They enable competitive performance on downstream tasks even with frozen encoders  
- They require significantly more parameters than supervised models

<details>
<summary>Show Answer</summary>

**Correct Answer:** They enable competitive performance on downstream tasks even with frozen encoders  
**Explanation:**  
Contrastive methods achieve strong results with fixed feature extractors.  
> "...you get extremely strong performance, even when you freeze the features and only train a linear classifier..."
</details>

