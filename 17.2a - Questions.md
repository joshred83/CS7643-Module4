# Quiz: Markov Decision Processes (MDPs)

---

### Question 1 (Multiple Choice)
What is the primary purpose of defining a Markov Decision Process (MDP) in reinforcement learning?

- To directly implement a robotic control algorithm  
- To provide a theoretical framework for sequential supervised learning  
- To serve as the foundational model for reinforcement learning  
- To replace policies with reward functions  

---

### Question 2 (Multiple Select)
Which of the following are components of a standard MDP definition?

- A reward function  
- A value function  
- A discount factor  
- A transition probability distribution  
- A Q-function  

---

### Question 3 (True/False)
The Markov property implies that the next state depends on the entire history of states and actions.

- True  
- False  

---

### Question 4 (Multiple Choice)
What kind of feedback is available to an agent in an MDP during learning?

- Full reward tables and state transitions  
- Only supervised training labels  
- Samples from the reward and transition distributions  
- Deterministic outcome matrices  

---

### Question 5 (Multiple Select)
Which of the following accurately describe a deterministic or stochastic policy?

- A deterministic policy maps states to actions  
- A stochastic policy maps actions to states  
- A stochastic policy defines a probability distribution over actions  
- A deterministic policy stores one action per state  

---

### Question 6 (Multiple Choice)
What does the discount factor \( \gamma \) influence in an MDP?

- The number of available actions per state  
- Whether the environment is deterministic  
- How much weight is given to future rewards  
- The memory capacity of the policy  

---

### Question 7 (True/False)
Lower values of the discount factor \( \gamma \) cause the agent to prioritize short-term rewards.

- True  
- False  

---

### Question 8 (Multiple Select)
Which of the following are value-related functions defined in the lecture?

- State-value function (V-function)  
- Reward matrix  
- State-action value function (Q-function)  
- Advantage function  

---

### Question 9 (Multiple Choice)
What happens when the reward at non-absorbing states is made more negative?

- The agent becomes more cautious and avoids all rewards  
- The optimal policy becomes independent of the environment  
- The agent tends to prefer shorter or riskier paths to reach absorbing states  
- The discount factor becomes irrelevant  

---

### Question 10 (True/False)
A Q-function provides an estimate of future rewards for a given action taken at a particular state.

- True  
- False  
