# Answer Key: Algorithms for Solving MDPs



### Question 1  
**Correct Answer:** True  
**Explanation:**  
The optimal value function is defined as the max over all Q-values at that state.  
> "The first says that the optimal value at a state is the same as the max Q value over possible actions at that state."

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Deep Q-Learning avoids full for-loops and instead uses function approximation and minibatches.  
> "Instead of having a for loop over all states to update the Q-network, as was done in Q-iteration, we introduced a regression objective..."
</details>

---



### Question 2  
**Correct Answer:** A recursive definition of the optimal value function  
**Explanation:**  
The Bellman equation defines value functions recursively based on future values.  
> "Taking a closer look at the definition of the optimal Q function, we will now try to rewrite it recursively..."  
> "The recursive Bellman equation derived so far will form the basis for... value iteration."

<details>
<summary>Show Answer</summary>

**Correct Answer:** It provides a recursive objective for value prediction  
**Explanation:**  
The Bellman equation is used to generate Q-value targets recursively.  
> "The update for our Q-network will again be inspired, by the recursive bellman optimality equation."
</details>

---



### Question 3  
**Correct Answer:** To iteratively converge on the optimal value function  
**Explanation:**  
Value iteration repeatedly applies Bellman updates until convergence.  
> "The central idea is to update this vector at each iteration by repeatedly applying this recursive Bellman equation until convergence."

<details>
<summary>Show Answer</summary>

**Correct Answers:** Gradient descent updates, A fixed replay buffer for data sampling, A target Q-network updated periodically  
**Explanation:**  
Training involves minibatch gradient descent, a replay buffer, and two networks (Qnew and Qold).  
> "We introduced a regression objective..."  
> "Two copies of the Q-network are maintained... Qold and Qnew..."  
> "Deep Q-learning employs an experience replay buffer..."
</details>

---



### Question 4  
**Correct Answers:** A series of value vectors \( V_0, V_1, \ldots \), Updates using the recursive Bellman equation, Time complexity that scales with the product of number of states and actions  
**Explanation:**  
The algorithm builds up sequences of value vectors, applies Bellman updates, and has quadratic time complexity.  
> "This update will produce a sequence of vectors V0, V1, and so on..."  
> "...by repeatedly applying this recursive Bellman equation..."  
> "Each iteration of this algorithm will have a time complexity of order of n square m..."

<details>
<summary>Show Answer</summary>

**Correct Answer:** To stabilize training by decoupling target predictions  
**Explanation:**  
Using a separate Qold network helps reduce instability in loss updates.  
> "Using a single Q-network, makes the loss minimization unstable... Instead, two copies... are maintained..."
</details>

---



### Question 5  
**Correct Answer:** False  
**Explanation:**  
Although policy iteration involves a policy evaluation step, it often converges faster.  
> "...the policy converges to pi star much sooner than the value converges to V of pi star, thus requiring fewer iterations."

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Two Q-networks are maintained: one for predictions (Qnew) and one as a fixed target (Qold).  
> "Qnew parameters are updated while preventing any update to the Qold parameters."
</details>

---



### Question 6  
**Correct Answer:** Q iteration operates directly on the Q-function  
**Explanation:**  
Q iteration works by updating Q-values directly instead of value vectors.  
> "We can derive an update rule for Q functions, which will form the basis of the Q iteration algorithm."

<details>
<summary>Show Answer</summary>

**Correct Answers:** It can prevent exploration of better strategies, It causes training data to be highly correlated  
**Explanation:**  
Using a greedy data collection policy limits exploration and causes data bias.  
> "...it will not have incentive to explore other less rewarding states..."  
> "The data... will be highly correlated with similar states, actions and rewards."
</details>

---



### Question 7  
**Correct Answer:** True  
**Explanation:**  
Policy iteration alternates between computing value estimates and performing greedy updates.  
> "The policy iteration algorithm involves two parts... compute V pi... then greedily update the policy."

<details>
<summary>Show Answer</summary>

**Correct Answer:** It balances exploration and exploitation during data gathering  
**Explanation:**  
Epsilon-greedy selects random actions occasionally to encourage exploration.  
> "...a random action is chosen with a typically small epsilon probability, and the greedy action is selected otherwise."
</details>

---



### Question 8  
**Correct Answers:** They require summing over all states and actions, The number of states in environments like Atari or chess is extremely large  
**Explanation:**  
Dynamic programming is computationally expensive and not feasible for large state/action spaces.  
> "...time complexity of one iteration update..."  
> "...chess... our lower bound being 10 to the power 420 states. And for Atari Games... the number of such images is also exponentially large."

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Replay buffers store a range of past experiences, not just recent ones, to reduce correlation.  
> "The buffer is a finite size and older samples are discarded in favor of newer ones... to lower the correlation..."
</details>

---



### Question 9  
**Correct Answer:** To choose the action that maximizes the expected value at each state  
**Explanation:**  
The greedy step in policy improvement involves selecting the action with maximum expected value.  
> "This greedy step involves picking the action that maximizes the value obtained at all states..."

<details>
<summary>Show Answer</summary>

**Correct Answers:** Using two separate Q-networks, Sampling random minibatches from past experiences  
**Explanation**
</details>

---



### Question 10  
**Correct Answer:** True  
**Explanation:**  
The lecture makes clear that these algorithms are built upon the Bellman optimality equations.  
> "We derived the recursive Bellman optimality equations that form the backbone of the three dynamic programming algorithms..."

<details>
<summary>Show Answer</summary>

Answer not found
</details>

