# Answer Key: Introduction to Unsupervised, Semi-Supervised, and Few-Shot Learning

---

### Question 1  
**Correct Answer:** False  
**Explanation:**  
Few-shot learning typically assumes access to a small auxiliary labeled dataset, not unlabeled data.  
> "In few-shot learning... we have only 1 to 5 examples per category. In the vanilla setting, we also don't have any unlabeled data..."

---

### Question 2  
**Correct Answer:** Combine a small amount of labeled data with a large amount of unlabeled data  
**Explanation:**  
Semi-supervised learning leverages limited labeled examples and a much larger set of unlabeled ones.  
> "In semi-supervised learning... we also have a large set of unlabeled data..."

---

### Question 3  
**Correct Answers:** Density estimation, Clustering  
**Explanation:**  
These are listed as classic unsupervised tasks.  
> "...purely unsupervised learning where we can perform tasks, such as clustering or density estimation."

---

### Question 4  
**Correct Answer:** To generate labeled examples from confident predictions on unlabeled data  
**Explanation:**  
Pseudo labels are used to augment the labeled dataset by converting high-confidence predictions into training targets.  
> "We can then take that and convert it into a pseudo label... and put them into the training set and retrain."

---

### Question 5  
**Correct Answer:** False  
**Explanation:**  
Self-supervised learning does not use human-annotated labels but instead creates tasks from unlabeled data.  
> "In self-supervised learning... we assume we have no labels. All we have are unlabeled examples..."

---

### Question 6  
**Correct Answers:** Selecting appropriate loss functions, Designing surrogate tasks that are neither too easy nor too hard  
**Explanation:**  
The lecture highlights challenges in selecting suitable losses and task difficulty.  
> "What loss functions should we use?"  
> "...surrogate tasks that hopefully aren't too easy... and aren't too hard."

---

### Question 7  
**Correct Answer:** A task that is designed to force the network to learn representations, even though the task itself is artificial  
**Explanation:**  
Surrogate tasks are artificial tasks designed to promote good feature learning.  
> "...we'll rotate an image... and try to make the neural network predict which rotations we applied... we actually don't care about it... but we're trying to force the neural network to learn..."

---

### Question 8  
**Correct Answer:** False  
**Explanation:**  
Metric learning can be done without labeled pairwise supervision by learning similarity functions from data.  
> "Or there are methods to do what's called metric learning where you try to learn a distance metric as well."

---

### Question 9  
**Correct Answers:** Using strong and weak augmentations, Cross-entropy loss with hard pseudo labels, Using data to self-generate soft targets for knowledge distillation  
**Explanation:**  
These techniques are all discussed as part of the semi-supervised learning pipeline.  
> "...perform augmentation of two types... weak form... strong augmentation..."  
> "We can take these confident pseudo labels... and retrain."  
> "...use what we've learned... in the form of knowledge distillation..."

---

### Question 10  
**Correct Answer:** To learn an initialization that adapts well to new tasks with few training steps  
**Explanation:**  
Meta-learning focuses on learning to adapt efficiently from very few examples.  
> "...learning an initialization that after you go through such process will be effective..."

