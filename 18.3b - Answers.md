# Answer Key: Few-Shot Learning

---

### Question 1  
**Correct Answer:** False  
**Explanation:**  
Few-shot learning operates under the constraint of very limited labeled examples per class.  
> "We only have a few examples called a support set, typically we only have somewhere on the range of 1 to 5 examples per category..."

---

### Question 2  
**Correct Answer:** To simulate test conditions using tasks constructed from base classes  
**Explanation:**  
Meta-training builds tasks that mirror the test conditions to improve generalization.  
> "We'd like to align what we do during training with what we do during testing... This is called meta-training..."

---

### Question 3  
**Correct Answers:** Freezing the feature extractor, Using cosine classifiers instead of fully connected layers, Applying early stopping and hyperparameter tuning  
**Explanation:**  
These methods help avoid overfitting when training on tiny support sets.  
> "...freeze the feature extractor... use various tricks such as Early Stopping..."  
> "...cosine or similarity-based classifiers... might perform better..."

---

### Question 4  
**Correct Answer:** Their angular separation (directional similarity)  
**Explanation:**  
Cosine classifiers measure the angle (not magnitude) between vectors.  
> "You’re only looking at the angles between the feature vectors rather than incorporating how long they are..."

---

### Question 5  
**Correct Answers:** Feature extractor shared across support and query sets, Mean embeddings of each class in the support set  
**Explanation:**  
Prototypical networks use class prototypes computed as the mean of embeddings.  
> "...you take the mean of those and then you compare each query item to that mean..."

---

### Question 6  
**Correct Answer:** False  
**Explanation:**  
Unlike matching networks, prototypical networks compare queries to class prototypes, not each support instance.  
> "...match it to a prototype or mean embedding..."

---

### Question 7  
**Correct Answer:** By maximizing cosine similarity with the support set embeddings  
**Explanation:**  
Matching networks make predictions based on similarity (e.g., cosine) with the support examples.  
> "...use a cosine similarity or distance function and essentially make predictions..."

---

### Question 8  
**Correct Answers:** It lacks task-specific awareness during meta-training, It is sensitive to hyperparameter choices  
**Explanation:**  
Standard fine-tuning doesn't account for the N-way test setup and can be finicky.  
> "...this does not factor the task into account that we will encounter during query or inference time..."  
> "...if you can do this in an effective manner... find the right hyperparameters..."

---

### Question 9  
**Correct Answer:** The initialization of parameters for rapid adaptation  
**Explanation:**  
MAML learns an initialization that performs well after minimal gradient updates.  
> "...we want to learn some initial set of weights that after performing gradient descent... doesn’t overfit..."

---

### Question 10  
**Correct Answer:** True  
**Explanation:**  
The computation of gradient descent can be unrolled as a differentiable graph.  
> "We can actually view gradient descent as a computation graph..."

---

### Question 11  
**Correct Answer:** A learnable update rule conditioned on gradients  
**Explanation:**  
Meta-LSTM learns how to update weights from gradients,
