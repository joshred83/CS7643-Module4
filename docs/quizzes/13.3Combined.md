# Quiz: Generative Adversarial Networks (GANs)

### Question 1 (True/False)
GANs learn an explicit probability density function over the input data \( p(x) \).

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>
**Correct Answers:** B
GANs do not learn an explicit density function. They belong to the category of implicit density models that learn to generate samples without explicitly modeling the probability distribution.

> "Generative adversarial networks or GANS did not learn an explicit density function p of x, rather they fit under the implicit density category."
</details>

---

---

### Question 2 (Multi-Select)

Which of the following accurately describe the role and characteristics of the discriminator in a GAN? (Select all that apply)

- [ ] A. To distinguish between real and generated samples

- [ ] B. To provide gradient feedback to improve the generator

- [ ] C. To directly generate synthetic images

- [ ] D. To encode images for classification

- [ ] E. To act as an adaptive loss function for the generator

- [ ] F. To approximate the Jensen-Shannon divergence between distributions

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, E, F
The discriminator has several key roles in a GAN: it differentiates between real and fake samples (A); provides gradient feedback that helps the generator improve (B); functions as an adaptive loss function that evolves as the generator improves (E); and implicitly helps approximate the Jensen-Shannon divergence between the real and generated distributions (F). The discriminator does not generate images (C) or encode them for classification tasks (D) - these are not its primary functions in the GAN framework.

> "The key idea of generative adversarial networks is to have another network that distinguishes between real and generated or fake images."
> "The discriminator provides gradient information that allows the generator to improve over time."
</details>

---

---

### Question 3 (Multi-Select)

Which of the following challenges are associated with training GANs? (Select all that apply)

- [ ] A. Require labeled data

- [ ] B. Instability due to adversarial training

- [ ] C. Mode collapse

- [ ] D. Non-convex optimization difficulties

<details>
<summary>Show Answer</summary>
**Correct Answers:** B, C, D
GANs face several training challenges: the adversarial training process creates instability (B); mode collapse can occur where the generator fails to capture the full diversity of the data distribution (C); and the optimization involves non-convex objectives with complex neural networks (D). GANs do not require labeled data (A is incorrect) as they can be trained in an unsupervised manner.

> "The complex dynamics of learning will actually turn out to be quite difficult to train."  
> "You have all sorts of strange dynamics that can occur between the generator and discriminator."  
> "There's something called mode collapse where you can't capture all the modes of the distribution."  
> "We're using very deep complex neural networks that are non convex."
</details>

---

---

### Question 4 (True/False)

The generator in a GAN is updated using the same part of the loss function that affects the discriminator.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>
**Correct Answers:** B
While the discriminator is trained on both real and fake samples, the generator is only trained on its ability to fool the discriminator. The generator and discriminator use different components of the overall objective function.

> "Note that for the generator, only one part of this objective function is valid... gradients for the discriminator comes from both."
</details>

---

---

### Question 5 (Multi-Select)

Which of the following can be used as inputs and characteristics of the generator component in GANs? (Select all that apply)

- [ ] A. A vector sampled from a Gaussian distribution

- [ ] B. Random noise from a uniform distribution

- [ ] C. A real image

- [ ] D. A label vector for conditional generation

- [ ] E. The generator maps from a simple distribution to a complex one

- [ ] F. The generator requires pre-training on autoencoder objectives

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, D, E
The generator in a GAN typically takes random noise as input, commonly from a Gaussian (A) or uniform (B) distribution. In conditional GANs, label vectors can also be used as input (D). The generator's role is to map from a simple distribution to a complex one (E). However, generators don't require pre-training on autoencoder objectives (F) and don't typically take real images as direct inputs (C).

> "We're going to use a simple idea of first sampling from a simple distribution, say a Gaussian... then feed it through a neural network."
> "The generator transforms points from a simple noise distribution into the complex data distribution through a neural network mapping."
> "In conditional GANs, we can provide additional information like class labels to guide the generation process."
</details>

---

---

### Question 6 (Multi-Select)

Which of the following are methods used to stabilize GAN training? (Select all that apply)

- [ ] A. Batch normalization

- [ ] B. Removing fully connected layers

- [ ] C. Freezing the discriminator weights

- [ ] D. Using LeakyReLU in the discriminator

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, D
Several architectural and training innovations have been developed to stabilize GAN training: batch normalization helps control the distribution of activations (A); replacing fully connected layers with convolutional layers improves stability (B); using LeakyReLU in the discriminator prevents gradient saturation (D). Freezing discriminator weights (C) is not a standard practice for stabilizing training.

> "Several findings were developed. For example, replacing pooling layers with strided convolution, using batchnorm both in the generator and discriminator, removing the fully connected hidden layers... using LeakyReLU activations for the discriminator."
</details>

---

---

### Question 7 (Multi-Select)

Which of the following accurately describe the objectives and optimization dynamics of the generator in a GAN? (Select all that apply)

- [ ] A. To fool the discriminator into thinking generated images are real

- [ ] B. To minimize the likelihood that the discriminator correctly identifies fake samples

- [ ] C. To minimize pixel-wise reconstruction error

- [ ] D. To maximize the probability that generated samples are classified as real

- [ ] E. To learn the mapping from latent space to data space

- [ ] F. To compute gradients on real data only

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, D, E
The generator's objectives include: fooling the discriminator into classifying generated images as real (A); minimizing the likelihood of the discriminator correctly identifying fake samples (B); maximizing the probability that generated samples are classified as real by the discriminator (D); and learning an effective mapping from the latent space to the data space (E). Unlike autoencoders, GANs don't use pixel-wise reconstruction error (C), and the generator computes gradients on fake rather than real data (F).

> "That means that it taking fake data and giving it a probability of being real as 1, which is what the generator wants because it wants to fool the discriminator."
> "The generator is trying to maximize the log probability of the discriminator making a mistake."
> "The generator learns the mapping from the latent space to a particular data distribution."
</details>

---

---

### Question 8 (True/False)

After training a GAN, only the generator is typically used to sample new data.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
Once training is complete, only the generator is needed to produce new samples. The discriminator has served its purpose in training the generator and is no longer required for the generation process.

> "At the end of the training, we'll have an implicit generator model... We actually don't need the discriminator for this process, we can just throw it away."
</details>

---

---

### Question 9 (Multi-Select)

Which of the following accurately describe mode collapse and related challenges in GAN training? (Select all that apply)

- [ ] A. The generator produces a small set of nearly identical outputs

- [ ] B. The generator fails to capture the full diversity of the training distribution

- [ ] C. The discriminator fails to distinguish real and fake samples

- [ ] D. The generator learns to generate only the most common patterns in the data

- [ ] E. Mode collapse can be diagnosed by observing low diversity in generated samples

- [ ] F. Mode collapse is unrelated to the minimax optimization process

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, D, E
Mode collapse involves the generator producing a limited set of similar outputs (A), failing to capture the full diversity of the data distribution (B), focusing on common patterns while ignoring rarer ones (D), and can be detected by observing low diversity in generated samples (E). It is related to the minimax optimization process (F is incorrect), and doesn't involve the discriminator failing to distinguish samples (C) - in fact, the discriminator may be very good at its task.

> "There's something called mode collapse... you're not truly sampling from the distribution, you're just outputting the samples that you already have."
> "The generator simply isn't able to represent all modes of the data distribution, leading to a lack of diversity in the generated samples."
</details>

---

---

### Question 10 (Multi-Select)

What are some real-world applications or extensions of GANs mentioned in the lecture? (Select all that apply)

- [ ] A. Video generation

- [ ] B. Audio synthesis

- [ ] C. Domain adaptation using adversarial losses

- [ ] D. Direct supervised classification with explicit \( p(x) \)

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, C
GANs have been extended to numerous applications: generating videos by modeling temporal dynamics (A); synthesizing realistic audio waveforms (B); and performing domain adaptation with adversarial losses (C). GANs do not perform direct supervised classification with explicit modeling of p(x) (D) as they are generative models that learn implicit distributions.

> "These days other adaptations have been developed to produce audio waveforms..."  
> "You can then take as input a 10 second footage of you... generate an entire video of you dancing..."  
> "The notion of adversarial losses... have been used successfully."
</details>