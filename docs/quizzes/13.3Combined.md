# Quiz: Generative Adversarial Networks (GANs)

### Question 1 (True/False)
GANs learn an explicit probability density function over the input data \( p(x) \).

- [ ] True
- [ ] False

<details>
<summary>S1how Answer</summary>

**Correct Answer:** False  
**Justification:**  
> "Generative adversarial networks or GANS did not learn an explicit density function p of x, rather they fit under the implicit density category."
</details>

---

### Question 2 (Multi-Select)
Which of the following accurately describe the role and characteristics of the discriminator in a GAN? (Select all that apply)

- [ ] To distinguish between real and generated samples
- [ ] To provide gradient feedback to improve the generator
- [ ] To directly generate synthetic images
- [ ] To encode images for classification
- [ ] To act as an adaptive loss function for the generator
- [ ] To approximate the Jensen-Shannon divergence between distributions

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ To distinguish between real and generated samples, ✅ To provide gradient feedback to improve the generator, ✅ To act as an adaptive loss function, ✅ To approximate the JS divergence  
**Justification:**  
> "The key idea of generative adversarial networks is to have another network that distinguishes between real and generated or fake images."
</details>

---

### Question 3 (Multiple Select)
Which of the following challenges are associated with training GANs?

- [ ] Require labeled data
- [ ] Instability due to adversarial training
- [ ] Mode collapse
- [ ] Non-convex optimization difficulties

<details>
<summary>Show Answer</summary>

**Correct Answers:** Instability due to adversarial training, Mode collapse, Non-convex optimization difficulties  
**Justification:**  
> "The complex dynamics of learning will actually turn out to be quite difficult to train."  
> "You have all sorts of strange dynamics that can occur between the generator and discriminator."  
> "There's something called mode collapse where you can't capture all the modes of the distribution."  
> "We're using very deep complex neural networks that are non convex."
</details>

---

### Question 4 (True/False)
The generator in a GAN is updated using the same part of the loss function that affects the discriminator.

- [ ] True
- [ ] False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Justification:**  
> "Note that for the generator, only one part of this objective function is valid... gradients for the discriminator comes from both."
</details>

---

### Question 5 (Multiple Choice)
### Question 5 (Multi-Select)
Which of the following can be used as inputs and characteristics of the generator component in GANs? (Select all that apply)

- [ ] A vector sampled from a Gaussian distribution
- [ ] Random noise from a uniform distribution
- [ ] A real image
- [ ] A label vector for conditional generation
- [ ] The generator maps from a simple distribution to a complex one
- [ ] The generator requires pre-training on autoencoder objectives

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Vector from Gaussian distribution, ✅ Random noise from uniform distribution, ✅ Label vector for conditional generation, ✅ Maps from simple to complex distribution  
**Justification:**  
> "We're going to use a simple idea of first sampling from a simple distribution, say a Gaussian... then feed it through a neural network."
> "The generator transforms points from a simple noise distribution into the complex data distribution through a neural network mapping."
> "In conditional GANs, we can provide additional information like class labels to guide the generation process."
</details>

---
Which of the following are methods used to stabilize GAN training?

- [ ] Batch normalization
- [ ] Removing fully connected layers
- [ ] Freezing the discriminator weights
- [ ] Using LeakyReLU in the discriminator

<details>
<summary>Show Answer</summary>

**Correct Answers:** Batch normalization, Removing fully connected layers, Using LeakyReLU in the discriminator  
**Justification:**  
> "Several findings were developed. For example, replacing pooling layers with strided convolution, using batchnorm both in the generator and discriminator, removing the fully connected hidden layers... using LeakyReLU activations for the discriminator."
</details>

---

### Question 7 (Multi-Select)
Which of the following accurately describe the objectives and optimization dynamics of the generator in a GAN? (Select all that apply)

- [ ] To fool the discriminator into thinking generated images are real
- [ ] To minimize the likelihood that the discriminator correctly identifies fake samples
- [ ] To minimize pixel-wise reconstruction error
- [ ] To maximize the probability that generated samples are classified as real
- [ ] To learn the mapping from latent space to data space
- [ ] To compute gradients on real data only

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Fool the discriminator, ✅ Minimize likelihood of correct identification, ✅ Maximize probability of classification as real, ✅ Learn mapping from latent to data space  
**Justification:**  
> "That means that it taking fake data and giving it a probability of being real as 1, which is what the generator wants because it wants to fool the discriminator."
> "The generator is trying to maximize the log probability of the discriminator making a mistake."
> "The generator learns the mapping from the latent space to a particular data distribution."
</details>

---

### Question 8 (True/False)
After training a GAN, only the generator is typically used to sample new data.

- [ ] True
- [ ] False

<details>
<summary>Show Answer</summary>

**Correct Answer:** True  
**Justification:**  
> "At the end of the training, we'll have an implicit generator model... We actually don't need the discriminator for this process, we can just throw it away."
</details>

---

### Question 9 (Multi-Select)
Which of the following accurately describe mode collapse and related challenges in GAN training? (Select all that apply)

- [ ] The generator produces a small set of nearly identical outputs
- [ ] The generator fails to capture the full diversity of the training distribution
- [ ] The discriminator fails to distinguish real and fake samples
- [ ] The generator learns to generate only the most common patterns in the data
- [ ] Mode collapse can be diagnosed by observing low diversity in generated samples
- [ ] Mode collapse is unrelated to the minimax optimization process

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Small set of nearly identical outputs, ✅ Fails to capture full diversity, ✅ Generates only common patterns, ✅ Can be diagnosed by low diversity  
**Justification:**  
> "There's something called mode collapse... you're not truly sampling from the distribution, you're just outputting the samples that you already have."
> "The generator simply isn't able to represent all modes of the data distribution, leading to a lack of diversity in the generated samples."
</details>

---

### Question 10 (Multiple Select)
What are some real-world applications or extensions of GANs mentioned in the lecture?

- [ ] Video generation
- [ ] Audio synthesis
- [ ] Domain adaptation using adversarial losses
- [ ] Direct supervised classification with explicit \( p(x) \)

<details>
<summary>Show Answer</summary>

**Correct Answers:** Video generation, Audio synthesis, Domain adaptation using adversarial losses  
**Justification:**  
> "These days other adaptations have been developed to produce audio waveforms..."  
> "You can then take as input a 10 second footage of you... generate an entire video of you dancing..."  
> "The notion of adversarial losses... have been used successfully."
</details>

