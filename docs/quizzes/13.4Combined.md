# Quiz: Variational Autoencoders (VAEs)

### Question 1 (True/False)
VAEs are generative models that maintain an explicit density model of the data.

- True
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** True  
**Explanation:**  
VAEs explicitly model the density function \( p(x) \), unlike GANs.  
> "Variational autoencoders, which again are explicit density models, but that which have approximate densities."
</details>

---

### Question 2 (Multi-Select)
Which of the following accurately describe the role and properties of the Variational Lower Bound (ELBO) in VAEs? (Select all that apply)

- It addresses the intractability of the marginal likelihood
- It allows for an indirect optimization of the log-likelihood
- It imposes a regularization effect on the latent space
- It enables direct sampling from the true posterior
- It provides a tractable objective function for optimization
- It completely eliminates the need for approximating the posterior

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Addresses intractability of marginal likelihood, ✅ Allows indirect optimization of log-likelihood, ✅ Imposes regularization effect, ✅ Provides tractable objective function  
**Explanation:**  
The ELBO is used because computing the marginal likelihood directly is intractable, and it provides a lower bound that can be optimized.  
> "Now, if we could directly maximize this, then we're essentially maximizing the likelihood... But we can't really do this. The integral doesn't allow us because it's intractable. Instead, what we're going to do is maximize what's called a variational lower bound..."
> "The KL divergence term in the ELBO acts as a regularizer that constrains the approximate posterior to be close to the prior."
</details>

---

### Question 3 (Multiple Select)
Which components are part of the VAE architecture?

- Encoder  
- Discriminator  
- Decoder  
- Variational objective  
- Contrastive loss

<details>
<summary>Show Answer</summary>

**Correct Answers:** Encoder, Decoder, Variational objective  
**Explanation:**  
VAEs consist of an encoder and decoder, trained using a variational lower bound (ELBO).  
> "We'll have an encoder... a decoder... and a variational lower bound that we can compute."
</details>

---

### Question 4 (Multi-Select)
Which of the following accurately describe the purpose and effects of the KL divergence term in the VAE loss function? (Select all that apply)

- It penalizes deviation of the learned distribution from the prior
- It acts as a regularizer on the latent space
- It ensures the latent space has a well-defined probability density
- It measures how well images are reconstructed
- It encourages a smooth, continuous latent space
- It forces the encoder to learn a specific fixed mapping

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Penalizes deviation from the prior, ✅ Acts as a regularizer, ✅ Ensures well-defined probability density, ✅ Encourages smooth continuous latent space  
**Explanation:**  
The KL term ensures that the learned latent distribution does not deviate too far from the prior (typically Gaussian), which regularizes the latent space.  
> "The second part of the term here, is a KL divergence between Q of z given x and p of z... And so we're taking the KL divergence between the Z's that our encoder network outputs and the prior..."
> "This regularization ensures that the latent space has meaningful properties that allow for sampling and interpolation."
</details>

---

### Question 5 (True/False)
The reparameterization trick enables backpropagation through the sampling process in VAEs.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** True  
**Explanation:**  
The reparameterization trick allows the model to be differentiable despite sampling.  
> "The problem is you can't actually back propagate through sampling... So there's something called a reparameterization trick... which allows you to do the sampling."
</details>

---

### Question 6 (Multi-Select)
Which of the following accurately describe the properties and outputs of the decoder in a VAE? (Select all that apply)

- The decoder outputs parameters of a Gaussian distribution
- The decoder maps from latent space back to data space
- The decoder models the conditional probability p(x|z)
- The decoder uses a one-hot encoding for all outputs
- The decoder typically includes mean and variance parameters
- The decoder's parameters are learned through gradient descent

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Outputs parameters of a Gaussian distribution, ✅ Maps from latent to data space, ✅ Models conditional probability p(x|z), ✅ Includes mean and variance parameters, ✅ Parameters learned through gradient descent  
**Explanation:**  
The decoder outputs the mean and (diagonal) covariance of a Gaussian from which samples are drawn, mapping from latent to data space.  
> "This decoder models p of x given z... it will be a Gaussian distribution parameter. Here specifically, it will be mu and Sigma."
> "The decoder network transforms the latent representation back into the original data space and learns the parameters of the output distribution."
</details>

---

### Question 7 (Multi-Select)
Which of the following are accurate statements about the relationship between the true posterior p(z|x) and the approximate posterior q(z|x) in VAEs? (Select all that apply)

- The true posterior p(z|x) cannot be computed directly due to intractability
- The approximate posterior q(z|x) is typically modeled as a Gaussian distribution
- The encoder network directly outputs parameters for q(z|x)
- The KL divergence term minimizes the difference between q(z|x) and p(z)
- The true posterior would require marginalization over all possible data points
- The approximation error between q(z|x) and p(z|x) can be quantified

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ True posterior cannot be computed directly, ✅ Approximate posterior typically modeled as Gaussian, ✅ Encoder outputs parameters for q(z|x), ✅ KL term minimizes difference with prior, ✅ True posterior requires marginalization  
**Explanation:**  
The true posterior is intractable, requiring an approximate posterior that is typically modeled as a Gaussian.  
> "The right hand side is actually intractable. We can't compute this term. And so what we're going to do is... ignore it."
> "What we're doing is using Q of z given x to approximate P of z given x."
> "The encoder network produces parameters for the approximate posterior, typically modeled as a diagonal Gaussian."
</details>

---

### Question 8 (Multiple Select)
Which benefits or capabilities are associated with the latent space of a trained VAE?

- Smooth interpolation between data points  
- High-resolution image generation  
- Disentanglement of semantic attributes  
- Use in downstream tasks  
- Fully deterministic reconstruction

<details>
<summary>Show Answer</summary>

**Correct Answers:** Smooth interpolation between data points, Disentanglement of semantic attributes, Use in downstream tasks  
**Explanation:**  
Latent space in VAEs supports interpolation and often learns disentangled, useful representations.
</details>

---

### Question 9 (True/False)
The decoder in a VAE directly generates the reconstructed image without involving any probability distribution.

- True  
- False

<details>
<summary>Show Answer</summary>

Answer not found
</details>

---

### Question 10 (Multi-Select)
Which of the following accurately describe limitations or challenges associated with VAEs, particularly when compared to other generative models? (Select all that apply)

- VAE samples tend to be blurrier than GAN samples
- VAEs can struggle with complex, high-dimensional data
- The Gaussian assumption in the latent space can be limiting
- VAEs prioritize reconstruction over sample quality
- VAEs cannot generate samples at all
- The KL divergence term can lead to posterior collapse

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Samples tend to be blurrier, ✅ Can struggle with complex data, ✅ Gaussian assumption can be limiting, ✅ Prioritize reconstruction over sample quality, ✅ KL divergence can lead to posterior collapse  
**Explanation:**  
VAEs often produce blurrier outputs due to the probabilistic nature of their objective function and struggle with various limitations.  
> "VAEs can suffer from blurry outputs."
> "The Gaussian assumption in both the prior and approximate posterior can limit the expressivity of the model."
> "VAEs optimize for reconstruction quality which can come at the expense of sample quality."
</details>

