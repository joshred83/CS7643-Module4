# Quiz: Policy Gradients, Actor-Critic

### Question 1 (True/False)  



Policy-based reinforcement learning methods directly optimize a parameterized policy.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** A
**Explanation:**  
Policy-based methods directly parameterize and optimize a policy.  
> "Policy based methods on the other hand directly parameterize a policy and optimize it to maximize returns."
</details>

---

---

---




### Question 2 (Multi-Select)  



Which of the following accurately describe differences between value-based and policy-based reinforcement learning methods? (Select all that apply)

- [ ] A. Value-based methods optimize value functions, while policy-based methods optimize policies directly

- [ ] B. Policy-based methods can naturally handle continuous action spaces, unlike basic value-based methods

- [ ] C. Value-based methods typically use epsilon-greedy exploration, while policy-based methods can learn stochastic policies

- [ ] D. Policy-based methods learn a Q-function before training a policy

- [ ] E. Value-based methods indirectly derive policies (e.g., greedy with respect to Q), while policy-based methods represent policies explicitly

- [ ] F. Value-based methods require discretization of continuous action spaces

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B
**Explanation:**  
Value-based and policy-based methods differ in several fundamental ways.  
> "Among the types of methods using RL value based methods learn q functions... Policy based methods... directly parameterize a policy..."
> "Policy-based methods have some advantages... they can naturally handle continuous action spaces."
> "Value-based methods derive policies indirectly, typically by being greedy with respect to the learned value function."
</details>

---

---

---




### Question 3 (Multiple Select)  



Which of the following are components of the REINFORCE algorithm?

- [ ] A. Sampling trajectories using the current policy

- [ ] B. Computing gradients with respect to a value function

- [ ] C. Applying the log-derivative trick to compute the gradient

- [ ] D. Using a fixed reward function for supervised updates

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
REINFORCE involves sampling data and using the log-derivative trick to compute gradients.  
> "We simply need to collect a small batch of trajectories using the current pi theta..."  
> "...in order to apply a relation known as the log derivative trick..."
</details>

---

---

---




### Question 4 (Multi-Select)  



Which of the following accurately describe the purposes and benefits of using the log-derivative trick in policy gradient methods? (Select all that apply)

- [ ] A. To simplify the integral of the gradient of a probability density

- [ ] B. To convert a complex gradient computation into an expectation that can be sampled

- [ ] C. To avoid computing derivatives of transition dynamics

- [ ] D. To enable gradient estimation without knowing the environment model

- [ ] E. To facilitate policy optimization via stochastic gradient ascent

- [ ] F. To ensure the Q-function remains differentiable

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B
**Explanation:**  
The log-derivative trick has several important benefits in policy gradient algorithms.  
> "...we can write the expectation as an integral... and apply a relation known as the log derivative trick..."
> "This allows us to express the gradient in terms of an expectation, which we can then estimate by sampling."
> "The policy gradient theorem provides a way to compute policy gradients without knowing the dynamics of the environment."
</details>

---

---

---




### Question 5 (True/False)  



In REINFORCE, the gradient update increases the likelihood of action sequences that result in higher rewards.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** A
**Explanation:**  
Gradient updates push the probability of successful actions higher.  
> "...the gradient update will push the probability of the chosen actions to be either higher or lower depending on the sign and magnitude of the total reward..."
</details>

---

---

---




### Question 6 (Multiple Select)  



What are some limitations of REINFORCE as discussed in the lecture?

- [ ] A. It cannot compute gradients for deterministic policies

- [ ] B. It suffers from high variance in gradient estimates

- [ ] C. It requires knowledge of transition probabilities

- [ ] D. It does not assign credit to individual actions within a trajectory

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
REINFORCE suffers from variance and lacks fine-grained credit assignment.  
> "...we are left with a coarse level feedback for the entire sequence..."  
> "...New variants of this algorithm have been proposed that aim to reduce the variance..."
</details>

---

---

---




### Question 7 (Multi-Select)  



Which of the following accurately describe the roles and properties of baselines in policy gradient algorithms? (Select all that apply)

- [ ] A. They reduce variance in gradient estimation

- [ ] B. They preserve the expected value of the gradient

- [ ] C. They can be state-dependent functions

- [ ] D. They are often implemented as value function approximations

- [ ] E. They ensure deterministic transitions

- [ ] F. They help address the credit assignment problem in long trajectories

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B
**Explanation:**  
Baselines serve several important purposes in policy gradient methods.  
> "...subtracting some baseline... will preserve the mean... while possibly reducing the variance..."
> "We can use a state-dependent baseline such as a value function approximation."
> "A good baseline helps distinguish which actions were actually better than average, addressing the credit assignment problem."
</details>

---

---

---




### Question 8 (True/False)  



Subtracting a baseline that does not depend on actions changes the mean of the policy gradient estimate.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** B
**Explanation:**  
Subtracting a baseline that does not depend on action preserves the mean.  
> "...subtracting some baseline... will preserve the mean of the gradient expectation..."
</details>

---

---

---




### Question 9 (Multi-Select)  



Which of the following accurately describe scaling factors and signal reductions used in different actor-critic variant algorithms? (Select all that apply)

- [ ] A. The advantage function (Q minus V) is used in Advantage Actor-Critic

- [ ] B. TD error can be used as an unbiased estimate of the advantage

- [ ] C. State-value functions serve as baselines for variance reduction

- [ ] D. A combination of n-step returns and value estimates can be used

- [ ] E. Importance sampling weights are applied when using off-policy data

- [ ] F. Raw rewards without any baseline are optimal for stable training

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B
**Explanation:**  
Actor-critic methods use various signals and scaling factors to improve performance.  
> "...advantage actor-critic that substitutes the reward with the advantage... defined as the Q function minus the V function."
> "TD error can be used as an unbiased sample of the advantage function."
> "State-value functions provide effective baselines that reduce variance while preserving the expected gradient."
</details>

---

---

---




### Question 10 (Multiple Select)  



Which elements are used in the actor-critic algorithm?

- [ ] A. A learned Q-function

- [ ] B. Policy gradient updates

- [ ] C. Advantage estimates

- [ ] D. A reward baseline derived from the policyâ€™s V-function

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
Actor-Critic methods include all of these elements as discussed.  
> "...actor-critic algorithm that replaces rewards with the Q function..."  
> "...advantage actor-critic... defined as the Q function minus the V function."
</details>

