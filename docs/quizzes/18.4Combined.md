# Quiz: Unsupervised and Self-Supervised Learning

### Question 1 (True/False)  



Unsupervised learning requires labels to compute the loss functions used for training.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** B
**Explanation:**  
Unsupervised learning does not rely on labels; it uses patterns in the data.  
> "We assume we have no labels, all we have are unlabeled examples..."
</details>

---

---

---


### Question 2 (Multiple Choice)  



What is the primary purpose of autoencoders in unsupervised learning?

- [ ] A. To classify input data using pseudo-labels

- [ ] B. To reconstruct labeled outputs from input data

- [ ] C. To extract low-dimensional representations that capture important features

- [ ] D. To optimize for high-resolution image synthesis

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
Autoencoders compress data into lower-dimensional representations for reconstruction.  
> "...we're trying to compress the image or any other data through some bottleneck..."
</details>

---

---

---


### Question 3 (Multiple Select)  



Which of the following are traditional tasks associated with unsupervised learning?

- [ ] A. Clustering

- [ ] B. Dimensionality reduction

- [ ] C. Reinforcement signal propagation

- [ ] D. Density estimation

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
These are core classical tasks under unsupervised learning.  
> "...tasks such as clustering or density estimation..."  
> "...use an autoencoder to perform dimensionality reduction..."
</details>

---

---

---


### Question 4 (Multiple Choice)  



What does the “bottleneck” in an autoencoder architecture encourage?

- [ ] A. Overfitting to high-dimensional features

- [ ] B. Learning trivial identity functions

- [ ] C. Compression of inputs into meaningful low-dimensional embeddings

- [ ] D. Increased accuracy through deep convolutional filters

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
The bottleneck enforces feature learning through constrained representation.  
> "...compress the image... and then decompress it again..."
</details>

---

---

---


### Question 5 (True/False)  



Autoencoders can be trained without any labels, using only the input data.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** A
**Explanation:**  
Autoencoders learn from reconstruction loss, not labels.  
> "...we train the autoencoder to reconstruct the original image..."
</details>

---

---

---


### Question 6 (Multiple Select)  



What are characteristics of an effective feature space learned via unsupervised learning?

- [ ] A. High density clusters for semantically similar inputs

- [ ] B. Widely dispersed examples regardless of semantic meaning

- [ ] C. Low-density regions separating conceptually distinct inputs

- [ ] D. Spatial locality preserved through convolutional filters

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
Effective representations cluster similar inputs and separate dissimilar ones.  
> "...samples that are semantically similar are close together..."  
> "...semantically dissimilar examples are far apart..."
</details>

---

---

---


### Question 7 (Multiple Choice)  



In clustering-based unsupervised learning pipelines, what are pseudo-labels?

- [ ] A. Randomly assigned labels for evaluation

- [ ] B. Labels assigned by a pretrained supervised classifier

- [ ] C. Cluster assignments used as training targets for backpropagation

- [ ] D. Binary indicators for supervised similarity

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
Clustering assigns pseudo-labels which are then used as targets.  
> "...use clustering algorithms to generate pseudo-labels for the unlabeled data..."
</details>

---

---

---


### Question 8 (Multiple Select)  



Which are potential issues when clustering is used to generate pseudo-labels?

- [ ] A. Empty clusters

- [ ] B. Trivial solutions where all items are grouped into one cluster

- [ ] C. Perfectly balanced partitions that prevent overfitting

- [ ] D. Inability to use backpropagation through clustering

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
These are known risks in clustering-based training.  
> "...there may be empty clusters or trivial solutions where all data collapse to one cluster..."
</details>

---

---

---


### Question 9 (True/False)  



Surrogate tasks are designed to train models using freely available labels from data transformations.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** A
**Explanation:**  
Surrogate tasks create labels from data transformations.  
> "...a surrogate task... where we try to predict the rotation... even though the original image has no label."
</details>

---

---

---


### Question 10 (Multiple Choice)  



Why is predicting image rotation considered a useful surrogate task?

- [ ] A. It trains the network to detect object boundaries precisely

- [ ] B. It forces the model to recognize semantic content of images

- [ ] C. It directly improves classification performance on downstream tasks

- [ ] D. It encodes time-based sequence relationships

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
Rotation prediction forces semantic awareness.  
> "...predict which of the four rotations... the neural network must understand whether there is an upright dog..."
</details>

---

---

---


### Question 11 (Multiple Select)  



Which of the following are common surrogate tasks for self-supervised learning?

- [ ] A. Colorization

- [ ] B. Jigsaw puzzle solving

- [ ] C. Rotation prediction

- [ ] D. GAN-based style transfer

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
These are classic self-supervised tasks.  
> "...predict the correct color... jigsaw puzzles... rotation prediction..."
</details>

---

---

---


### Question 12 (True/False)  



The quality of learned representations is typically evaluated by training a complex nonlinear classifier on top of the encoder.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** B
**Explanation:**  
Representations are evaluated using a simple linear classifier to avoid confounding effects.  
> "We will train a linear classifier on top of the features... and test the accuracy..."
</details>

---

---

---


### Question 13 (Multiple Choice)  



What is the goal of instance discrimination in contrastive learning?

- [ ] A. Maximize diversity of the input images

- [ ] B. Align representations of augmentations of the same image while separating others

- [ ] C. Match latent codes across different modalities

- [ ] D. Use nearest neighbors for label propagation

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
Instance discrimination aims to align augmentations and separate distinct inputs.  
> "...make the representations of the augmentations of the same image close together... other examples farther apart..."
</details>

---

---

---


### Question 14 (Multiple Select)  



Which techniques improve the efficiency of instance discrimination training?

- [ ] A. Using mini-batch negatives

- [ ] B. Maintaining a memory bank of encoded features

- [ ] C. Sampling only hard negatives manually

- [ ] D. Using an exponential moving average to stabilize key encoders

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
All of these are techniques used to improve training in contrastive learning.  
> "...use mini-batch negatives... use a memory bank... exponential moving average of the encoder weights..."
</details>

---

---

---


### Question 15 (Multiple Choice)  



What is a key advantage of self-supervised representations learned via contrastive learning?

- [ ] A. They generalize poorly but train quickly

- [ ] B. They replace labeled data with GAN-generated features

- [ ] C. They enable competitive performance on downstream tasks even with frozen encoders

- [ ] D. They require significantly more parameters than supervised models

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
**Explanation:**  
Contrastive methods achieve strong results with fixed feature extractors.  
> "...you get extremely strong performance, even when you freeze the features and only train a linear classifier..."
</details>

