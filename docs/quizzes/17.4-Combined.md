# Quiz: -

# Deep Q-Learning Quiz





### Question 1 (Multi-Select)


Which of the following statements correctly describe key differences between Deep Q-Learning and traditional Q-iteration? (Select all that apply)

- [ ] A. Deep Q-Learning uses for-loops to iterate through all possible states

- [ ] B. Deep Q-Learning employs function approximation via neural networks

- [ ] C. Deep Q-Learning uses minibatch gradient descent updates

- [ ] D. Deep Q-Learning requires enumerating the entire state space

- [ ] E. Deep Q-Learning uses regression objectives rather than dynamic programming

- [ ] F. Deep Q-Learning updates all states simultaneously in each iteration

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Deep Q-Learning avoids full for-loops and instead uses function approximation and minibatches with regression objectives.  
> "Instead of having a for loop over all states to update the Q-network, as was done in Q-iteration, we introduced a regression objective..."
> "In practice, we will compute the loss for a minibatch of size B, instead of the entire data set."
</details>



### Question 2 (Multi-Select)


Which of the following describe how the Bellman equation is used in Deep Q-Learning? (Select all that apply)

- [ ] A. It provides a recursive objective for value prediction

- [ ] B. It eliminates the need for neural networks

- [ ] C. It enables the calculation of target Q-values for network updates

- [ ] D. It guarantees convergence to global optima

- [ ] E. It helps define the mean squared error loss function

- [ ] F. It creates a mathematical connection between current and future states

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
The Bellman equation is used to generate Q-value targets recursively and forms the basis of the MSE loss function by connecting current state values to future state values.  
> "The update for our Q-network will again be inspired, by the recursive bellman optimality equation."
> "During training, we can use a single Q-network to predict the Q-values for the current state and action shown on the left, and the next state and next actions shown in blue on the right."
> "Intuitively, this will attempt to make the predicted Q-values in red, match the target Q-values on the right."
</details>



### Question 3 (Multiple Choice)

Which of the following are key components of Deep Q-Learning? (Select all that apply)

- [ ] A. Gradient descent updates

- [ ] B. A fixed replay buffer for data sampling

- [ ] C. A target Q-network updated periodically

- [ ] D. State enumeration across the entire state space

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Training involves minibatch gradient descent, a replay buffer, and two networks (Qnew and Qold).  
> "We introduced a regression objective..."  
> "Two copies of the Q-network are maintained... Qold and Qnew..."  
> "Deep Q-learning employs an experience replay buffer..."
</details>



### Question 4 (Multi-Select)


Which of the following accurately describe the purpose and implementation of the dual Q-network architecture (Qold and Qnew) in Deep Q-Learning? (Select all that apply)

- [ ] A. To stabilize training by decoupling target predictions from current network updates

- [ ] B. To reduce instability in the loss optimization process

- [ ] C. To periodically synchronize network parameters for consistent learning

- [ ] D. To double the learning capacity of the model

- [ ] E. To enable learning from target values that don't shift during optimization

- [ ] F. To reduce memory requirements during training

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Using a separate Qold network helps reduce instability by keeping target values fixed during updates and periodically synchronizing the networks.  
> "Using a single Q-network, makes the loss minimization unstable... Instead, two copies... are maintained..."
> "Qnew parameters are updated while preventing any update to the Qold parameters."
> "Then, at regular intervals, the Qold network receives a fresh copy of the parameters from the Qnew network."
</details>



### Question 5 (True/False)

In Deep Q-Learning, both Q-networks (Qold and Qnew) are updated simultaneously during training.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Two Q-networks are maintained: one for predictions (Qnew) and one as a fixed target (Qold).  
> "Qnew parameters are updated while preventing any update to the Qold parameters."
</details>



### Question 6 (Multiple Choice)

What are potential issues with using a purely greedy policy for collecting training data? (Select all that apply)

- [ ] A. It can prevent exploration of better strategies

- [ ] B. It causes training data to be highly correlated

- [ ] C. It requires too much computation

- [ ] D. It makes the algorithm too random

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Using a greedy data collection policy limits exploration and causes data bias.  
> "...it will not have incentive to explore other less rewarding states..."  
> "The data... will be highly correlated with similar states, actions and rewards."
</details>



### Question 7 (Multiple Choice)

What is the purpose of the epsilon-greedy approach in Deep Q-Learning?

- [ ] A. It balances exploration and exploitation during data gathering

- [ ] B. It reduces network size

- [ ] C. It accelerates convergence to local minima

- [ ] D. It avoids the need for a replay buffer

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Epsilon-greedy selects random actions occasionally to encourage exploration.  
> "...a random action is chosen with a typically small epsilon probability, and the greedy action is selected otherwise."
</details>



### Question 8 (True/False)

The experience replay buffer in Deep Q-Learning primarily stores the most recent experiences for training.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**  
Replay buffers store a range of past experiences, not just recent ones, to reduce correlation.  
> "The buffer is a finite size and older samples are discarded in favor of newer ones... to lower the correlation..."
</details>



### Question 9 (Multiple Choice)

Which techniques help stabilize Deep Q-Learning training? (Select all that apply)

- [ ] A. Using two separate Q-networks

- [ ] B. Sampling random minibatches from past experiences

- [ ] C. Manually setting network weights

- [ ] D. Avoiding neural networks entirely

<details>
<summary>Show Answer</summary>

**Correct Answers:** 
**Explanation:**
Using two networks decouples target computation from current predictions, while random minibatch sampling from the replay buffer reduces correlations in the training data.
</details>