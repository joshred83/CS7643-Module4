# Deep Q-Learning Quiz

### Question 1 (Multi-Select)
Which of the following statements correctly describe key differences between Deep Q-Learning and traditional Q-iteration? (Select all that apply)

- [ ] Deep Q-Learning uses for-loops to iterate through all possible states
- [ ] Deep Q-Learning employs function approximation via neural networks
- [ ] Deep Q-Learning uses minibatch gradient descent updates
- [ ] Deep Q-Learning requires enumerating the entire state space
- [ ] Deep Q-Learning uses regression objectives rather than dynamic programming
- [ ] Deep Q-Learning updates all states simultaneously in each iteration

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Employs function approximation, ✅ Uses minibatch gradient descent, ✅ Uses regression objectives  
**Explanation:**  
Deep Q-Learning avoids full for-loops and instead uses function approximation and minibatches with regression objectives.  
> "Instead of having a for loop over all states to update the Q-network, as was done in Q-iteration, we introduced a regression objective..."
> "In practice, we will compute the loss for a minibatch of size B, instead of the entire data set."
</details>

---

### Question 2 (Multi-Select)
Which of the following describe how the Bellman equation is used in Deep Q-Learning? (Select all that apply)

- [ ] It provides a recursive objective for value prediction
- [ ] It eliminates the need for neural networks
- [ ] It enables the calculation of target Q-values for network updates
- [ ] It guarantees convergence to global optima
- [ ] It helps define the mean squared error loss function
- [ ] It creates a mathematical connection between current and future states

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Provides a recursive objective, ✅ Enables calculation of target Q-values, ✅ Helps define the MSE loss function, ✅ Creates connection between current and future states  
**Explanation:**  
The Bellman equation is used to generate Q-value targets recursively and forms the basis of the MSE loss function by connecting current state values to future state values.  
> "The update for our Q-network will again be inspired, by the recursive bellman optimality equation."
> "During training, we can use a single Q-network to predict the Q-values for the current state and action shown on the left, and the next state and next actions shown in blue on the right."
> "Intuitively, this will attempt to make the predicted Q-values in red, match the target Q-values on the right."
</details>

---

### Question 3
Which of the following are key components of Deep Q-Learning? (Select all that apply)

- [ ] Gradient descent updates
- [ ] A fixed replay buffer for data sampling
- [ ] A target Q-network updated periodically
- [ ] State enumeration across the entire state space

<details>
<summary>Show Answer</summary>

**Correct Answers:** Gradient descent updates, A fixed replay buffer for data sampling, A target Q-network updated periodically  
**Explanation:**  
Training involves minibatch gradient descent, a replay buffer, and two networks (Qnew and Qold).  
> "We introduced a regression objective..."  
> "Two copies of the Q-network are maintained... Qold and Qnew..."  
> "Deep Q-learning employs an experience replay buffer..."
</details>

---

### Question 4 (Multi-Select)
Which of the following accurately describe the purpose and implementation of the dual Q-network architecture (Qold and Qnew) in Deep Q-Learning? (Select all that apply)

- [ ] To stabilize training by decoupling target predictions from current network updates
- [ ] To reduce instability in the loss optimization process
- [ ] To periodically synchronize network parameters for consistent learning
- [ ] To double the learning capacity of the model
- [ ] To enable learning from target values that don't shift during optimization
- [ ] To reduce memory requirements during training

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Stabilize training by decoupling predictions, ✅ Reduce instability in loss optimization, ✅ Periodically synchronize parameters, ✅ Enable learning from non-shifting targets  
**Explanation:**  
Using a separate Qold network helps reduce instability by keeping target values fixed during updates and periodically synchronizing the networks.  
> "Using a single Q-network, makes the loss minimization unstable... Instead, two copies... are maintained..."
> "Qnew parameters are updated while preventing any update to the Qold parameters."
> "Then, at regular intervals, the Qold network receives a fresh copy of the parameters from the Qnew network."
</details>

---

### Question 5
In Deep Q-Learning, both Q-networks (Qold and Qnew) are updated simultaneously during training.

- [ ] True
- [ ] False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Two Q-networks are maintained: one for predictions (Qnew) and one as a fixed target (Qold).  
> "Qnew parameters are updated while preventing any update to the Qold parameters."
</details>

---

### Question 6
What are potential issues with using a purely greedy policy for collecting training data? (Select all that apply)

- [ ] It can prevent exploration of better strategies
- [ ] It causes training data to be highly correlated
- [ ] It requires too much computation
- [ ] It makes the algorithm too random

<details>
<summary>Show Answer</summary>

**Correct Answers:** It can prevent exploration of better strategies, It causes training data to be highly correlated  
**Explanation:**  
Using a greedy data collection policy limits exploration and causes data bias.  
> "...it will not have incentive to explore other less rewarding states..."  
> "The data... will be highly correlated with similar states, actions and rewards."
</details>

---

### Question 7
What is the purpose of the epsilon-greedy approach in Deep Q-Learning?

- [ ] It balances exploration and exploitation during data gathering
- [ ] It reduces network size
- [ ] It accelerates convergence to local minima
- [ ] It avoids the need for a replay buffer

<details>
<summary>Show Answer</summary>

**Correct Answer:** It balances exploration and exploitation during data gathering  
**Explanation:**  
Epsilon-greedy selects random actions occasionally to encourage exploration.  
> "...a random action is chosen with a typically small epsilon probability, and the greedy action is selected otherwise."
</details>

---

### Question 8
The experience replay buffer in Deep Q-Learning primarily stores the most recent experiences for training.

- [ ] True
- [ ] False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Replay buffers store a range of past experiences, not just recent ones, to reduce correlation.  
> "The buffer is a finite size and older samples are discarded in favor of newer ones... to lower the correlation..."
</details>

---

### Question 9
Which techniques help stabilize Deep Q-Learning training? (Select all that apply)

- [ ] Using two separate Q-networks
- [ ] Sampling random minibatches from past experiences
- [ ] Manually setting network weights
- [ ] Avoiding neural networks entirely

<details>
<summary>Show Answer</summary>

**Correct Answers:** Using two separate Q-networks, Sampling random minibatches from past experiences  
**Explanation:**
Using two networks decouples target computation from current predictions, while random minibatch sampling from the replay buffer reduces correlations in the training data.
</details>