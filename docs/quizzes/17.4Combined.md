# Quiz: Deep Q-Learning

### Question 1 (True/False)

Deep Q-Learning uses a for-loop over all states to update the Q-function, similar to traditional Q-iteration.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>
**Correct Answers:** B
Deep Q-Learning avoids full for-loops and instead uses function approximation and minibatches.

> "Instead of having a for loop over all states to update the Q-network, as was done in Q-iteration, we introduced a regression objective..."
</details>

---

---

### Question 2 (Multi-Select)

What role does the Bellman equation play in Deep Q-Learning? (Select all that apply)

- [ ] A. It provides a recursive objective for value prediction

- [ ] B. It helps determine which states to sample in each batch

- [ ] C. It is used to construct the loss function for neural network training

- [ ] D. It ensures that Q-values are bounded between 0 and 1

- [ ] E. It defines how to update the agent's policy

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, C
The Bellman equation is used to generate Q-value targets recursively (A) and forms the basis of the loss function for training the Q-network (C). It doesn't determine which states to sample (B), doesn't bound Q-values between 0 and 1 (D), and doesn't directly define policy updates (E) - these happen through greedy action selection.

> "The update for our Q-network will again be inspired, by the recursive bellman optimality equation."
</details>

---

---

### Question 3 (Multi-Select)

Which of the following are key components of the Deep Q-Learning training approach? (Select all that apply)

- [ ] A. Gradient descent updates

- [ ] B. A fixed replay buffer for data sampling

- [ ] C. A target Q-network updated periodically

- [ ] D. Importance sampling for prioritized replay

- [ ] E. Actor and critic networks

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, C
Training involves minibatch gradient descent (A), a replay buffer for sampling past experiences (B), and two networks (Qnew and Qold) with the target network updated periodically (C). Importance sampling for prioritized replay (D) is an extension but not part of the basic algorithm, and actor-critic networks (E) are used in other RL approaches.

> "We introduced a regression objective..."  
> "Two copies of the Q-network are maintained... Qold and Qnew..."  
> "Deep Q-learning employs an experience replay buffer..."
</details>

---

---

### Question 4 (Multiple Choice)

What is the purpose of using a separate target network (Qold) in Deep Q-Learning?

- [ ] A. To stabilize training by decoupling target predictions

- [ ] B. To enable parallel computation across multiple processors

- [ ] C. To reduce memory usage during training

- [ ] D. To allow for different learning rates at different stages

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
Using a separate Qold network helps reduce instability in loss updates by keeping target values fixed while updating the main network. This decoupling prevents the "moving target" problem where both predictions and targets shift simultaneously.

> "Using a single Q-network, makes the loss minimization unstable... Instead, two copies... are maintained..."
</details>

---

---

### Question 5 (True/False)

In Deep Q-Learning, both Q-networks (Qnew and Qold) are updated simultaneously during training.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>
**Correct Answers:** B
Only the Qnew network is continuously updated through gradient descent. The Qold network is kept fixed for a number of iterations and then periodically updated with the weights from Qnew.

> "Qnew parameters are updated while preventing any update to the Qold parameters."
</details>

---

---

### Question 6 (Multi-Select)

What problems can arise when collecting training data using a purely greedy policy in Deep Q-Learning? (Select all that apply)

- [ ] A. It can prevent exploration of better strategies

- [ ] B. It causes training data to be highly correlated

- [ ] C. It may lead to catastrophic forgetting

- [ ] D. It can result in overestimation of Q-values

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B
Using a greedy data collection policy limits exploration of potentially better actions (A) and causes consecutive training samples to be highly correlated (B), which can harm neural network training. While catastrophic forgetting (C) and Q-value overestimation (D) are challenges in RL, they're not directly caused by greedy data collection.

> "...it will not have incentive to explore other less rewarding states..."  
> "The data... will be highly correlated with similar states, actions and rewards."
</details>

---

---

### Question 7 (Multiple Choice)

What is the main purpose of the epsilon-greedy strategy in Deep Q-Learning?

- [ ] A. To reduce computational complexity

- [ ] B. To balance exploration and exploitation during data gathering

- [ ] C. To compensate for overestimation bias

- [ ] D. To gradually decrease learning rates over time

<details>
<summary>Show Answer</summary>
**Correct Answers:** B
The epsilon-greedy strategy allows for occasional random action selection (exploration) while still leveraging the learned Q-values for maximizing rewards (exploitation). This balance is crucial for finding optimal policies.

> "...a random action is chosen with a typically small epsilon probability, and the greedy action is selected otherwise."
</details>

---

---

### Question 8 (True/False)

In a replay buffer, only the most recent experiences are stored to ensure the agent learns from the latest environment dynamics.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>
**Correct Answers:** B
Replay buffers store a mix of recent and older experiences, not just the most recent ones. While older experiences may eventually be discarded, the buffer maintains a diverse set of samples to reduce correlation between training examples.

> "The buffer is a finite size and older samples are discarded in favor of newer ones... to lower the correlation..."
</details>

---

---

### Question 9 (Multi-Select)

Which techniques help stabilize Deep Q-Learning training? (Select all that apply)

- [ ] A. Using two separate Q-networks

- [ ] B. Sampling random minibatches from past experiences

- [ ] C. Clipping rewards to a fixed range

- [ ] D. Normalizing state inputs

- [ ] E. Limiting the number of training steps

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, C, D
Several techniques help stabilize training: using separate target and online networks (A), sampling random minibatches to break correlation (B), clipping rewards to manage scale (C), and normalizing inputs to improve neural network training (D). Limiting training steps (E) isn't typically used for stabilization.

> "Two copies of the Q-network are maintained... Qold and Qnew..."
> "...samples a random minibatch of size m from the replay buffer..."
> "Preprocessing is vital for the successful training of deep Q-learning agents."
</details>

---

---

### Question 10 (Multiple Choice)

What is a key challenge when applying Deep Q-Learning to environments with continuous action spaces?

- [ ] A. The Q-function must be evaluated for infinitely many actions

- [ ] B. Neural networks cannot approximate continuous functions

- [ ] C. The replay buffer becomes inefficient

- [ ] D. Reward signals become too sparse

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
In continuous action spaces, finding the maximum Q-value requires evaluating infinitely many actions, which is computationally infeasible. This limitation makes basic Deep Q-Learning better suited for discrete action spaces.

> "If our action space is continuous, choosing the best action becomes an optimization problem in its own right, because the arg max step requires enumerating all possible actions."
</details>