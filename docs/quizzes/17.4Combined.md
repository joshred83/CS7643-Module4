# Quiz: 

# Answer Key: Algorithms for Solving MDPs





### Question 1 (Multiple Choice)

**Correct Answer:*

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Deep Q-Learning avoids full for-loops and instead uses function approximation and minibatches.  
> "Instead of having a for loop over all states to update the Q-network, as was done in Q-iteration, we introduced a regression objective..."
</details>

---





### Question 2 (Multiple Choice)

**Correct Answer:*

<details>
<summary>Show Answer</summary>

**Correct Answer:** It provides a recursive objective for value prediction  
**Explanation:**  
The Bellman equation is used to generate Q-value targets recursively.  
> "The update for our Q-network will again be inspired, by the recursive bellman optimality equation."
</details>

---





### Question 3 (Multiple Choice)

**Correct Answer:*

<details>
<summary>Show Answer</summary>

**Correct Answers:** Gradient descent updates, A fixed replay buffer for data sampling, A target Q-network updated periodically  
**Explanation:**  
Training involves minibatch gradient descent, a replay buffer, and two networks (Qnew and Qold).  
> "We introduced a regression objective..."  
> "Two copies of the Q-network are maintained... Qold and Qnew..."  
> "Deep Q-learning employs an experience replay buffer..."
</details>

---





### Question 4 (Multiple Choice)

**Correct Answers:*

<details>
<summary>Show Answer</summary>

**Correct Answer:** To stabilize training by decoupling target predictions  
**Explanation:**  
Using a separate Qold network helps reduce instability in loss updates.  
> "Using a single Q-network, makes the loss minimization unstable... Instead, two copies... are maintained..."
</details>

---





### Question 5 (Multiple Choice)

**Correct Answer:*

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Two Q-networks are maintained: one for predictions (Qnew) and one as a fixed target (Qold).  
> "Qnew parameters are updated while preventing any update to the Qold parameters."
</details>

---





### Question 6 (Multiple Choice)

**Correct Answer:*

<details>
<summary>Show Answer</summary>

**Correct Answers:** It can prevent exploration of better strategies, It causes training data to be highly correlated  
**Explanation:**  
Using a greedy data collection policy limits exploration and causes data bias.  
> "...it will not have incentive to explore other less rewarding states..."  
> "The data... will be highly correlated with similar states, actions and rewards."
</details>

---





### Question 7 (Multiple Choice)

**Correct Answer:*

<details>
<summary>Show Answer</summary>

**Correct Answer:** It balances exploration and exploitation during data gathering  
**Explanation:**  
Epsilon-greedy selects random actions occasionally to encourage exploration.  
> "...a random action is chosen with a typically small epsilon probability, and the greedy action is selected otherwise."
</details>

---





### Question 8 (Multiple Choice)

**Correct Answers:*

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Replay buffers store a range of past experiences, not just recent ones, to reduce correlation.  
> "The buffer is a finite size and older samples are discarded in favor of newer ones... to lower the correlation..."
</details>

---





### Question 9 (Multiple Choice)

**Correct Answer:*

<details>
<summary>Show Answer</summary>

**Correct Answers:** Using two separate Q-networks, Sampling random minibatches from past experiences  
**Explanation**
</details>

---





### Question 10 (Multiple Choice)

**Correct Answer:*

<details>
<summary>Show Answer</summary>

**Correct Answers:** [Need to manually determine]
Answer not found
</details>

