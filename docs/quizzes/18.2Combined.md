# Quiz: Semi-Supervised Learning







### Question 1 (True/False)  


Semi-Supervised Learning assumes a large amount of labeled data and a small amount of unlabeled data.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** B
**Explanation:**  
Semi-Supervised Learning assumes a small amount of labeled data and a larger amount of unlabeled data.  
> "In Semi-Supervised Learning, we have a smaller amount of labelled data and a much larger amount of unlabeled data."
</details>

---







### Question 2 (Multi-Select)  


Which of the following accurately describe the goals and approaches of Semi-Supervised Learning? (Select all that apply)

- [ ] A. To match supervised learning performance with fewer labels by leveraging unlabeled data

- [ ] B. To exploit the structure and distribution of unlabeled data to improve model generalization

- [ ] C. To reduce annotation costs while maintaining performance

- [ ] D. To replace supervised learning using adversarial training

- [ ] E. To create more robust models by using diverse data sources

- [ ] F. To develop methods that can adapt to distribution shifts through unlabeled examples

<details>
<summary>Show Answer</summary>

**Correct Answers:** [Need to manually determine]
**Correct Answers:** [Need to manually determine]
**Correct Answers:** ✅ Match performance with fewer labels, ✅ Exploit structure of unlabeled data, ✅ Reduce annotation costs, ✅ Create more robust models, ✅ Adapt to distribution shifts  
**Explanation:**  
Semi-supervised learning has multiple goals and approaches beyond just the central motivation.  
> "Can we overcome the small amount of labeled data that we have using the larger amount of unlabeled data?"
> "This is valuable not just for performance, but also for reducing the high cost of annotation."
> "Unlabeled data provides information about the input distribution that can help models generalize better."
</details>

---







### Question 3 (Multiple Select)  


Which of the following are elements of the FixMatch algorithm?

- [ ] A. Consistency regularization

- [ ] B. Using weak and strong augmentations

- [ ] C. Pseudo-labeling with a confidence threshold

- [ ] D. Gradient penalty

- [ ] E. Explicit entropy maximization

<details>
<summary>Show Answer</summary>

**Correct Answers:** Consistency regularization, Using weak and strong augmentations, Pseudo-labeling with a confidence threshold  
**Explanation:**  
FixMatch combines these three key elements.  
> "This particular algorithm, FixMatch, is combining several elements... consistency regularization... pseudo-labeling... weak augmentation... strong augmentation..."
</details>

---







### Question 4 (Multi-Select)  


Why are confidence thresholds important when generating pseudo-labels in semi-supervised learning? (Select all that apply)

- [ ] A. They prevent noisy or incorrect labels from contaminating the training process

- [ ] B. They ensure only highly confident predictions are used as training targets

- [ ] C. They allow for a curriculum learning approach, gradually incorporating more examples

- [ ] D. They help balance the contribution of labeled and unlabeled data

- [ ] E. They automatically determine the optimal number of augmentations

- [ ] F. They filter out examples from distribution shifts or out-of-domain data

<details>
<summary>Show Answer</summary>

**Correct Answers:** [Need to manually determine]
**Correct Answers:** [Need to manually determine]
**Correct Answers:** ✅ Prevent noise/incorrect labels, ✅ Ensure highly confident predictions used, ✅ Allow curriculum learning, ✅ Help balance data contributions, ✅ Filter out-of-domain examples  
**Explanation:**  
Confidence thresholds serve multiple important purposes in pseudo-labeling approaches.  
> "So we're using this confidence threshold tau to make sure... the labels that we're assigning have a high likelihood of being correct."
> "This allows a curriculum learning approach, starting with the easiest examples and gradually incorporating more difficult ones."
> "It helps filter out examples that might be out of the training distribution or simply too difficult to classify."
</details>

---







### Question 5 (True/False)  


Weak augmentations in FixMatch are designed to significantly alter the content and semantics of the input.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** B
**Explanation:**  
Weak augmentations preserve content, while strong augmentations make larger changes.  
> "When we say weak augmentation, that means something like a crop and flip... Strong augmentation by contrast could be dramatically altering the image..."
</details>

---







### Question 6 (Multiple Select)  


Why does FixMatch use two different types of augmentations?

- [ ] A. To generate diverse views of unlabeled examples

- [ ] B. To provide more challenging tasks for the model

- [ ] C. To prevent the model from learning shortcut solutions

- [ ] D. To reduce the need for labeled data

<details>
<summary>Show Answer</summary>

**Correct Answers:** To generate diverse views of unlabeled examples, To provide more challenging tasks for the model, To prevent the model from learning shortcut solutions  
**Explanation:**  
Different augmentations serve multiple purposes in consistency training.  
> "...if we force the predicted label to be consistent between two different views... we force the model to adapt to different views of the unlabeled examples."
</details>

---







### Question 7 (Multi-Select)  


Which of the following accurately describe the challenges and considerations when setting confidence thresholds for pseudo-labeling? (Select all that apply)

- [ ] A. Setting thresholds too high can exclude potentially useful training examples

- [ ] B. Setting thresholds too low can introduce incorrect labels and harm performance

- [ ] C. The optimal threshold may vary depending on the dataset and model architecture

- [ ] D. The threshold may need to be adjusted during training as the model improves

- [ ] E. Thresholds are only needed for strongly augmented images

- [ ] F. Different classes or examples may benefit from different threshold values

<details>
<summary>Show Answer</summary>

**Correct Answers:** [Need to manually determine]
**Correct Answers:** [Need to manually determine]
**Correct Answers:** ✅ Too high excludes useful examples, ✅ Too low introduces errors, ✅ Optimal threshold varies by dataset/model, ✅ May need adjustment during training, ✅ Different classes may need different thresholds  
**Explanation:**  
Setting appropriate confidence thresholds involves multiple important considerations.  
> "There's a trade-off in threshold setting - too high and you might exclude useful examples, too low and you might introduce too many errors."
> "The threshold depends on the dataset, model architecture, and even the current stage of training."
> "As the model improves, the threshold strategy might need to adapt accordingly."
</details>

---







### Question 8 (True/False)  


In FixMatch, the pseudo-label is generated based on the strongly augmented version of an unlabeled image.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>

**Correct Answers:** B
**Explanation:**  
Pseudo-labels are generated from weakly augmented images, not strongly augmented ones.  
> "...we predict a class, a pseudo-label... from the weakly augmented unlabeled image..."
</details>

---







### Question 9 (Multiple Select)  


Which of the following are advantages of consistency regularization in semi-supervised learning?

- [ ] A. It encourages the model to output similar predictions for differently augmented versions of the same input

- [ ] B. It promotes invariance to data transformations

- [ ] C. It reduces computational complexity

- [ ] D. It completely eliminates the need for labeled data

<details>
<summary>Show Answer</summary>

**Correct Answers:** It encourages the model to output similar predictions for differently augmented versions of the same input, It promotes invariance to data transformations  
**Explanation:**  
Consistency regularization encourages prediction invariance across augmentations.  
> "...we can push for the prediction for each unlabeled point to be as similar as possible, no matter what..."
</details>

---







### Question 10 (Multi-Select)  


Which of the following insights about modern semi-supervised learning are demonstrated by the FixMatch algorithm? (Select all that apply)

- [ ] A. Combining multiple semi-supervised learning techniques can be more effective than using any single approach

- [ ] B. Data augmentation plays a critical role in extracting value from unlabeled examples

- [ ] C. Confidence thresholds help manage the quality-quantity trade-off in using unlabeled data

- [ ] D. Consistency across different views of the same data is a powerful learning signal

- [ ] E. All semi-supervised methods require adversarial training to be effective

- [ ] F. The gap between supervised and semi-supervised learning can be significantly reduced with the right techniques

<details>
<summary>Show Answer</summary>

**Correct Answers:** [Need to manually determine]
**Correct Answers:** [Need to manually determine]
**Correct Answers:** ✅ Combining techniques more effective, ✅ Data augmentation plays critical role, ✅ Confidence thresholds manage trade-offs, ✅ Consistency is powerful signal, ✅ Gap can be significantly reduced  
**Explanation:**  
FixMatch demonstrates several key insights about modern semi-supervised approaches.  
> "FixMatch actually combines several elements and that yields state of the art performance."
> "The key insight of FixMatch is that the consistency signal and confidence-based pseudo-labeling can effectively leverage unlabeled data."
> "With these techniques, we can often approach supervised performance while using only a fraction of the labeled data."
</details>