# Quiz: Semi-Supervised Learning

### Question 1 (True/False)  
Semi-Supervised Learning assumes a large amount of labeled data and a small amount of unlabeled data.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Semi-Supervised Learning assumes a small amount of labeled data and a larger amount of unlabeled data.  
> "In Semi-Supervised Learning, we have a smaller amount of labelled data and a much larger amount of unlabeled data."
</details>

---

### Question 2 (Multi-Select)  
Which of the following accurately describe the goals and approaches of Semi-Supervised Learning? (Select all that apply)

- To match supervised learning performance with fewer labels by leveraging unlabeled data
- To exploit the structure and distribution of unlabeled data to improve model generalization
- To reduce annotation costs while maintaining performance
- To replace supervised learning using adversarial training
- To create more robust models by using diverse data sources
- To develop methods that can adapt to distribution shifts through unlabeled examples

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Match performance with fewer labels, ✅ Exploit structure of unlabeled data, ✅ Reduce annotation costs, ✅ Create more robust models, ✅ Adapt to distribution shifts  
**Explanation:**  
Semi-supervised learning has multiple goals and approaches beyond just the central motivation.  
> "Can we overcome the small amount of labeled data that we have using the larger amount of unlabeled data?"
> "This is valuable not just for performance, but also for reducing the high cost of annotation."
> "Unlabeled data provides information about the input distribution that can help models generalize better."
</details>

---

### Question 3 (Multiple Select)  
Which of the following are elements of the FixMatch algorithm?

- Consistency regularization  
- Using weak and strong augmentations  
- Pseudo-labeling with a confidence threshold  
- Gradient penalty  
- Explicit entropy maximization

<details>
<summary>Show Answer</summary>

**Correct Answers:** Consistency regularization, Using weak and strong augmentations, Pseudo-labeling with a confidence threshold  
**Explanation:**  
FixMatch combines these three key elements.  
> "This particular algorithm, FixMatch, is combining several elements... consistency regularization... pseudo-labeling... weak augmentation... strong augmentation..."
</details>

---

### Question 4 (Multi-Select)  
Why are confidence thresholds important when generating pseudo-labels in semi-supervised learning? (Select all that apply)

- They prevent noisy or incorrect labels from contaminating the training process
- They ensure only highly confident predictions are used as training targets
- They allow for a curriculum learning approach, gradually incorporating more examples
- They help balance the contribution of labeled and unlabeled data
- They automatically determine the optimal number of augmentations
- They filter out examples from distribution shifts or out-of-domain data

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Prevent noise/incorrect labels, ✅ Ensure highly confident predictions used, ✅ Allow curriculum learning, ✅ Help balance data contributions, ✅ Filter out-of-domain examples  
**Explanation:**  
Confidence thresholds serve multiple important purposes in pseudo-labeling approaches.  
> "So we're using this confidence threshold tau to make sure... the labels that we're assigning have a high likelihood of being correct."
> "This allows a curriculum learning approach, starting with the easiest examples and gradually incorporating more difficult ones."
> "It helps filter out examples that might be out of the training distribution or simply too difficult to classify."
</details>

---

### Question 5 (True/False)  
Weak augmentations in FixMatch are designed to significantly alter the content and semantics of the input.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Weak augmentations preserve content, while strong augmentations make larger changes.  
> "When we say weak augmentation, that means something like a crop and flip... Strong augmentation by contrast could be dramatically altering the image..."
</details>

---

### Question 6 (Multiple Select)  
Why does FixMatch use two different types of augmentations?

- To generate diverse views of unlabeled examples  
- To provide more challenging tasks for the model  
- To prevent the model from learning shortcut solutions  
- To reduce the need for labeled data

<details>
<summary>Show Answer</summary>

**Correct Answers:** To generate diverse views of unlabeled examples, To provide more challenging tasks for the model, To prevent the model from learning shortcut solutions  
**Explanation:**  
Different augmentations serve multiple purposes in consistency training.  
> "...if we force the predicted label to be consistent between two different views... we force the model to adapt to different views of the unlabeled examples."
</details>

---

### Question 7 (Multi-Select)  
Which of the following accurately describe the challenges and considerations when setting confidence thresholds for pseudo-labeling? (Select all that apply)

- Setting thresholds too high can exclude potentially useful training examples
- Setting thresholds too low can introduce incorrect labels and harm performance
- The optimal threshold may vary depending on the dataset and model architecture
- The threshold may need to be adjusted during training as the model improves
- Thresholds are only needed for strongly augmented images
- Different classes or examples may benefit from different threshold values

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Too high excludes useful examples, ✅ Too low introduces errors, ✅ Optimal threshold varies by dataset/model, ✅ May need adjustment during training, ✅ Different classes may need different thresholds  
**Explanation:**  
Setting appropriate confidence thresholds involves multiple important considerations.  
> "There's a trade-off in threshold setting - too high and you might exclude useful examples, too low and you might introduce too many errors."
> "The threshold depends on the dataset, model architecture, and even the current stage of training."
> "As the model improves, the threshold strategy might need to adapt accordingly."
</details>

---

### Question 8 (True/False)  
In FixMatch, the pseudo-label is generated based on the strongly augmented version of an unlabeled image.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Pseudo-labels are generated from weakly augmented images, not strongly augmented ones.  
> "...we predict a class, a pseudo-label... from the weakly augmented unlabeled image..."
</details>

---

### Question 9 (Multiple Select)  
Which of the following are advantages of consistency regularization in semi-supervised learning?

- It encourages the model to output similar predictions for differently augmented versions of the same input  
- It promotes invariance to data transformations  
- It reduces computational complexity  
- It completely eliminates the need for labeled data

<details>
<summary>Show Answer</summary>

**Correct Answers:** It encourages the model to output similar predictions for differently augmented versions of the same input, It promotes invariance to data transformations  
**Explanation:**  
Consistency regularization encourages prediction invariance across augmentations.  
> "...we can push for the prediction for each unlabeled point to be as similar as possible, no matter what..."
</details>

---

### Question 10 (Multi-Select)  
Which of the following insights about modern semi-supervised learning are demonstrated by the FixMatch algorithm? (Select all that apply)

- Combining multiple semi-supervised learning techniques can be more effective than using any single approach
- Data augmentation plays a critical role in extracting value from unlabeled examples
- Confidence thresholds help manage the quality-quantity trade-off in using unlabeled data
- Consistency across different views of the same data is a powerful learning signal
- All semi-supervised methods require adversarial training to be effective
- The gap between supervised and semi-supervised learning can be significantly reduced with the right techniques

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Combining techniques more effective, ✅ Data augmentation plays critical role, ✅ Confidence thresholds manage trade-offs, ✅ Consistency is powerful signal, ✅ Gap can be significantly reduced  
**Explanation:**  
FixMatch demonstrates several key insights about modern semi-supervised approaches.  
> "FixMatch actually combines several elements and that yields state of the art performance."
> "The key insight of FixMatch is that the consistency signal and confidence-based pseudo-labeling can effectively leverage unlabeled data."
> "With these techniques, we can often approach supervised performance while using only a fraction of the labeled data."
</details>