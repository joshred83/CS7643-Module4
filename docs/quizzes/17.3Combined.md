# Quiz: Algorithms for Solving MDPs

### Question 1 (True/False)
The optimal value function \( V^* \) for a state is defined as the maximum over all actions of the corresponding optimal Q-values \( Q^*(s, a) \).

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** True  
**Explanation:**  
The optimal value function is defined as the max over all Q-values at that state.  
> "The first says that the optimal value at a state is the same as the max Q value over possible actions at that state."
</details>

---

### Question 2 (Multi-Select)
Which of the following accurately describe the properties and role of the Bellman equation in MDPs? (Select all that apply)

- It provides a recursive definition of the optimal value function
- It expresses the relationship between current and future state values
- It is the foundation for value iteration and policy iteration algorithms
- It enables efficient computation of optimal policies
- It replaces stochastic policies in reinforcement learning
- It defines value in terms of immediate rewards plus discounted future rewards

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Recursive definition of optimal value, ✅ Relationship between current and future values, ✅ Foundation for algorithms, ✅ Enables efficient computation, ✅ Defines value in terms of rewards  
**Explanation:**  
The Bellman equation has several key properties and roles in MDP solution methods.  
> "Taking a closer look at the definition of the optimal Q function, we will now try to rewrite it recursively..."  
> "The recursive Bellman equation derived so far will form the basis for... value iteration."
> "This recursive structure allows us to break down the complex problem of finding optimal policies into a series of simpler calculations."
</details>

---

### Question 3 (Multi-Select)
Which of the following accurately describe the characteristics, goals, and properties of the value iteration algorithm? (Select all that apply)

- It iteratively converges on the optimal value function
- It applies the Bellman equation recursively until convergence
- It has time complexity related to the number of states and actions
- It requires a fully known MDP model with transition probabilities
- It generates visualizations of the policy graph
- It maintains a sequence of improving value function estimates

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Iteratively converges on optimal value, ✅ Applies Bellman equation recursively, ✅ Time complexity related to states/actions, ✅ Requires known MDP model, ✅ Maintains sequence of improving estimates  
**Explanation:**  
Value iteration has several key properties beyond just its primary goal.  
> "The central idea is to update this vector at each iteration by repeatedly applying this recursive Bellman equation until convergence."
> "Each iteration of this algorithm will have a time complexity of order of n square m..."
> "This update will produce a sequence of vectors V0, V1, and so on..."
</details>

---

### Question 4 (Multiple Select)
Which of the following are components or results of value iteration?

- A series of value vectors \( V_0, V_1, \ldots \)  
- Updates using the recursive Bellman equation  
- Convergence to the optimal Q-function directly  
- Time complexity that scales with the product of number of states and actions

<details>
<summary>Show Answer</summary>

**Correct Answers:** A series of value vectors \( V_0, V_1, \ldots \), Updates using the recursive Bellman equation, Time complexity that scales with the product of number of states and actions  
**Explanation:**  
The algorithm builds up sequences of value vectors, applies Bellman updates, and has quadratic time complexity.  
> "This update will produce a sequence of vectors V0, V1, and so on..."  
> "...by repeatedly applying this recursive Bellman equation..."  
> "Each iteration of this algorithm will have a time complexity of order of n square m..."
</details>

---

### Question 5 (True/False)
Policy iteration always requires more iterations than value iteration to converge.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** False  
**Explanation:**  
Although policy iteration involves a policy evaluation step, it often converges faster.  
> "...the policy converges to pi star much sooner than the value converges to V of pi star, thus requiring fewer iterations."
</details>

---

### Question 6 (Multi-Select)
Which of the following accurately describe differences between value iteration and Q iteration algorithms? (Select all that apply)

- Q iteration operates directly on the Q-function while value iteration works with value functions
- Q iteration provides action values directly without needing additional computation
- Q iteration requires more memory but can make action selection more efficient
- Q iteration avoids any dependence on transition probabilities
- Value iteration cannot be used with function approximation
- Q iteration operates on state-action pairs while value iteration operates on states

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Q iteration operates directly on Q-function, ✅ Provides action values directly, ✅ Requires more memory but more efficient selection, ✅ Operates on state-action pairs vs states  
**Explanation:**  
There are several key differences between these algorithms beyond just the primary distinction.  
> "We can derive an update rule for Q functions, which will form the basis of the Q iteration algorithm."
> "While value iteration computes V(s), Q iteration computes Q(s,a) directly, which requires more memory but makes action selection more straightforward."
> "Q iteration operates on the larger space of state-action pairs rather than just states."
</details>

---

### Question 7 (True/False)
Policy iteration alternates between evaluating the current policy and greedily updating it.

- True  
- False

<details>
<summary>Show Answer</summary>

**Correct Answer:** True  
**Explanation:**  
Policy iteration alternates between computing value estimates and performing greedy updates.  
> "The policy iteration algorithm involves two parts... compute V pi... then greedily update the policy."
</details>

---

### Question 8 (Multiple Select)
Why are dynamic programming approaches to solving MDPs often impractical for large environments?

- They assume continuous action spaces  
- They require summing over all states and actions  
- They don’t converge for stochastic transitions  
- The number of states in environments like Atari or chess is extremely large

<details>
<summary>Show Answer</summary>

**Correct Answers:** They require summing over all states and actions, The number of states in environments like Atari or chess is extremely large  
**Explanation:**  
Dynamic programming is computationally expensive and not feasible for large state/action spaces.  
> "...time complexity of one iteration update..."  
> "...chess... our lower bound being 10 to the power 420 states. And for Atari Games... the number of such images is also exponentially large."
</details>

---

### Question 9 (Multi-Select)
Which of the following accurately describe the purpose and characteristics of the policy improvement step in policy iteration? (Select all that apply)

- It chooses the action that maximizes the expected value at each state
- It applies a greedy strategy with respect to the current value function
- It guarantees monotonic improvement of the policy
- It eventually leads to the optimal policy when combined with policy evaluation
- It samples new actions from a uniform distribution
- It can sometimes cause policy oscillation in stochastic environments

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Chooses action maximizing expected value, ✅ Applies greedy strategy, ✅ Guarantees monotonic improvement, ✅ Eventually leads to optimal policy  
**Explanation:**  
Policy improvement has several important properties beyond just the basic greedy selection.  
> "This greedy step involves picking the action that maximizes the value obtained at all states..."
> "Policy improvement guarantees that each new policy will be at least as good as the previous one."
> "The combination of policy evaluation and policy improvement will eventually converge to the optimal policy."
</details>

---

### Question 10 (Multi-Select)
Which of the following accurately describe the role and applications of Bellman equations in reinforcement learning algorithms? (Select all that apply)

- They form the theoretical foundation for value iteration, Q iteration, and policy iteration
- They provide a recursive formulation that enables dynamic programming approaches
- They allow breaking down a complex long-term planning problem into simpler subproblems
- They are only applicable to deterministic environments
- They enable calculation of optimal policies without knowledge of transition probabilities
- They define the relationship between current and future expected rewards

<details>
<summary>Show Answer</summary>

**Correct Answers:** ✅ Form foundation for algorithms, ✅ Provide recursive formulation, ✅ Break down complex problems, ✅ Define relationship between rewards  
**Explanation:**  
Bellman equations have multiple important roles and applications in RL algorithms.  
> "We derived the recursive Bellman optimality equations that form the backbone of the three dynamic programming algorithms..."
> "The recursive structure of the Bellman equations allows us to break down the complex problem of finding optimal long-term policies into a series of simpler, recursive subproblems."
> "These equations define the relationship between the value of a state and the values of its successor states."
</details>

