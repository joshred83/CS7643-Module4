# Quiz: Algorithms for Solving MDPs

### Question 1 (True/False)

The optimal value function \( V^* \) for a state is defined as the maximum over all actions of the corresponding optimal Q-values \( Q^*(s, a) \).

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
The optimal value function is defined as the max over all Q-values at that state.  
> "The first says that the optimal value at a state is the same as the max Q value over possible actions at that state."
</details>

---

---

---


### Question 2 (Multi-Select)


Which of the following accurately describe the properties and role of the Bellman equation in MDPs? (Select all that apply)

- [ ] A. It provides a recursive definition of the optimal value function

- [ ] B. It expresses the relationship between current and future state values

- [ ] C. It is the foundation for value iteration and policy iteration algorithms

- [ ] D. It enables efficient computation of optimal policies

- [ ] E. It replaces stochastic policies in reinforcement learning

- [ ] F. It defines value in terms of immediate rewards plus discounted future rewards

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, C, D, F
The Bellman equation has several key properties and roles in MDP solution methods.  
> "Taking a closer look at the definition of the optimal Q function, we will now try to rewrite it recursively..."  
> "The recursive Bellman equation derived so far will form the basis for... value iteration."
> "This recursive structure allows us to break down the complex problem of finding optimal policies into a series of simpler calculations."
</details>

---

---

---


### Question 3 (Multi-Select)


Which of the following accurately describe the characteristics, goals, and properties of the value iteration algorithm? (Select all that apply)

- [ ] A. It iteratively converges on the optimal value function

- [ ] B. It applies the Bellman equation recursively until convergence

- [ ] C. It has time complexity related to the number of states and actions

- [ ] D. It requires a fully known MDP model with transition probabilities

- [ ] E. It generates visualizations of the policy graph

- [ ] F. It maintains a sequence of improving value function estimates

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, C, D, F
Value iteration has several key properties beyond just its primary goal.  
> "The central idea is to update this vector at each iteration by repeatedly applying this recursive Bellman equation until convergence."
> "Each iteration of this algorithm will have a time complexity of order of n square m..."
> "This update will produce a sequence of vectors V0, V1, and so on..."
</details>

---

---

---


### Question 4 (Multiple Select)


Which of the following are components or results of value iteration?

- [ ] A. A series of value vectors \( V_0, V_1, \ldots \)

- [ ] B. Updates using the recursive Bellman equation

- [ ] C. Convergence to the optimal Q-function directly

- [ ] D. Time complexity that scales with the product of number of states and actions

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, D
The algorithm builds up sequences of value vectors, applies Bellman updates, and has quadratic time complexity.  
> "This update will produce a sequence of vectors V0, V1, and so on..."  
> "...by repeatedly applying this recursive Bellman equation..."  
> "Each iteration of this algorithm will have a time complexity of order of n square m..."
</details>

---

---

---


### Question 5 (True/False)


Policy iteration always requires more iterations than value iteration to converge.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>
**Correct Answers:** B
Although policy iteration involves a policy evaluation step, it often converges faster.  
> "...the policy converges to pi star much sooner than the value converges to V of pi star, thus requiring fewer iterations."
</details>

---

---

---


### Question 6 (Multi-Select)


Which of the following accurately describe differences between value iteration and Q iteration algorithms? (Select all that apply)

- [ ] A. Q iteration operates directly on the Q-function while value iteration works with value functions

- [ ] B. Q iteration provides action values directly without needing additional computation

- [ ] C. Q iteration requires more memory but can make action selection more efficient

- [ ] D. Q iteration avoids any dependence on transition probabilities

- [ ] E. Value iteration cannot be used with function approximation

- [ ] F. Q iteration operates on state-action pairs while value iteration operates on states

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, C, F
There are several key differences between these algorithms beyond just the primary distinction.  
> "We can derive an update rule for Q functions, which will form the basis of the Q iteration algorithm."
> "While value iteration computes V(s), Q iteration computes Q(s,a) directly, which requires more memory but makes action selection more straightforward."
> "Q iteration operates on the larger space of state-action pairs rather than just states."
</details>

---

---

---


### Question 7 (True/False)


Policy iteration alternates between evaluating the current policy and greedily updating it.

- [ ] A. True

- [ ] B. False

<details>
<summary>Show Answer</summary>
**Correct Answers:** A
Policy iteration alternates between computing value estimates and performing greedy updates.  
> "The policy iteration algorithm involves two parts... compute V pi... then greedily update the policy."
</details>

---

---

---


### Question 8 (Multiple Select)


Why are dynamic programming approaches to solving MDPs often impractical for large environments?

- [ ] A. They assume continuous action spaces

- [ ] B. They require summing over all states and actions

- [ ] C. They don't converge for stochastic transitions

- [ ] D. The number of states in environments like Atari or chess is extremely large

<details>
<summary>Show Answer</summary>
**Correct Answers:** B, D
Dynamic programming is computationally expensive and not feasible for large state/action spaces.  
> "...time complexity of one iteration update..."  
> "...chess... our lower bound being 10 to the power 420 states. And for Atari Games... the number of such images is also exponentially large."
</details>

---

---

---


### Question 9 (Multi-Select)


Which of the following accurately describe the purpose and characteristics of the policy improvement step in policy iteration? (Select all that apply)

- [ ] A. It chooses the action that maximizes the expected value at each state

- [ ] B. It applies a greedy strategy with respect to the current value function

- [ ] C. It guarantees monotonic improvement of the policy

- [ ] D. It eventually leads to the optimal policy when combined with policy evaluation

- [ ] E. It samples new actions from a uniform distribution

- [ ] F. It can sometimes cause policy oscillation in stochastic environments

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, C, D
Policy improvement has several important properties beyond just the basic greedy selection.  
> "This greedy step involves picking the action that maximizes the value obtained at all states..."
> "Policy improvement guarantees that each new policy will be at least as good as the previous one."
> "The combination of policy evaluation and policy improvement will eventually converge to the optimal policy."
</details>

---

---

---


### Question 10 (Multi-Select)


Which of the following accurately describe the role and applications of Bellman equations in reinforcement learning algorithms? (Select all that apply)

- [ ] A. They form the theoretical foundation for value iteration, Q iteration, and policy iteration

- [ ] B. They provide a recursive formulation that enables dynamic programming approaches

- [ ] C. They allow breaking down a complex long-term planning problem into simpler subproblems

- [ ] D. They are only applicable to deterministic environments

- [ ] E. They enable calculation of optimal policies without knowledge of transition probabilities

- [ ] F. They define the relationship between current and future expected rewards

<details>
<summary>Show Answer</summary>
**Correct Answers:** A, B, C, F
Bellman equations have multiple important roles and applications in RL algorithms.  
> "We derived the recursive Bellman optimality equations that form the backbone of the three dynamic programming algorithms..."
> "The recursive structure of the Bellman equations allows us to break down the complex problem of finding optimal long-term policies into a series of simpler, recursive subproblems."
> "These equations define the relationship between the value of a state and the values of its successor states."
</details>