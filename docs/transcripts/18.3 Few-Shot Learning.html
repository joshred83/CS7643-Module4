<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>18.3 Few-Shot Learning - CS7643 Module 4</title>
  <link rel="stylesheet" href="../css/styles.css">
</head>
<body>
  <div class="container">
    <div class="sidebar">
      <h2><a href="../index.html" style="color: white; text-decoration: none;">CS7643 Module 4</a></h2>
      <div class="topic-group">
        <h3>Generative Models</h3>
        <ul>
          <li><a href="13.1Combined.html">13.1 Introduction</a></li>
          <li><a href="13.2Combined.html">13.2 PixelRNN & PixelCNN</a></li>
          <li><a href="13.3Combined.html">13.3 GANs</a></li>
          <li><a href="13.4Combined.html">13.4 VAEs</a></li>
        </ul>
      </div>
      <div class="topic-group">
        <h3>Reinforcement Learning</h3>
        <ul>
          <li><a href="17.1Combined.html">17.1 Introduction</a></li>
          <li><a href="17.2Combined.html">17.2 MDPs</a></li>
          <li><a href="17.3Combined.html">17.3 Solving MDPs</a></li>
          <li><a href="17.4-Combined.html">17.4 Deep Q-Learning</a></li>
          <li><a href="17.5Combined.html">17.5 Policy Gradients</a></li>
        </ul>
      </div>
      <div class="topic-group">
        <h3>Advanced Topics</h3>
        <ul>
          <li><a href="18.1Combined.html">18.1 Introduction</a></li>
          <li><a href="18.2Combined.html">18.2 Semi-Supervised</a></li>
          <li><a href="18.3Combined.html">18.3 Few-Shot</a></li>
          <li><a href="18.4Combined.html">18.4 Self-Supervised</a></li>
        </ul>
      </div>
      <div class="topic-group">
        <h3>Transcripts</h3>
        <ul>
          <li><a href="../transcripts/index.html">View All Transcripts</a></li>
        </ul>
      </div>
    </div>
    
    <div class="content">
      
        <h2 class="section-title">18.3 Few-Shot Learning</h2>
        <div class="transcript-container">
          <div class="transcript">
            <p><span class="timestamp">[00:00:00]</span>
>> In this lesson we'll talk about few shot learning where you may only have a small number of examples pro category say one to 5 and we'd like to transfer from a larger label dataset in order to do well in this challenging scenario in fuchsia learning the studying is that we have lots of labels and a base set of categories and we have a lot of different categories as well somewhere on the range of hundreds and what we'd like to do is learn something that will allow us to learn effective classifiers for a new set of and classes where and is typically $5.00 to $20.00 and in rare cases a little bit more than that and so we'd like to generalize to this new and set of end classes but we only have a few examples called a support set typically we only have somewhere on the range of $1.00 to $5.00 examples per category for these and classes and enduring inference we have a query set where for each query item we'd like to match it to which of the end classes does it come from so this is called an n. way test and again and is typically somewhere from $5.00 to $20.00 categories so we're not really distinguishing between thousands of categories and this is because this is a challenging setting we only have a few examples per category and there's nothing special about the setting this is just how the original papers on a few shot learning set it up and it represents an interesting question of can we learn on the end plus one class something effective even though we only have a small number of examples leveraging the large label dataset on a different set of categories and again Recent research has broadened the setting in various ways that we won't cover here but there's nothing special about the setting this is just how it was originally set up there are several approaches to a few shot learning we can try to do what we always do that is fine tuning or transfer learning we can learn a model on the large base set of classes This includes both a feature extractor not a classifier at.</p><p><span class="timestamp">[00:02:00]</span>
And then we can optionally freeze the feature extractor if you remember typically we would like to do this when we only have a small number of labeled examples on the dataset that we're transferring to this is because we don't have enough examples to actually effectively optimize the features and so we only really have enough to learn some classifiers and then we learn the classifier waits for new classes using a small amount of data that is we take the support set which includes all of the classes and a few examples per class and then we just train as normal and we can use various tricks such as Early Stopping and so on in order to try to prevent over 50 this is actually a surprisingly effective baseline and over and over throughout the years people have shown that if you can do this in an effective manner that is find the right hyper parameters and training schedule and so on you can actually do quite well because we are only performing classification across a handful of categories remember and way test that we perform their inquiry time refers to somewhere between 5 to 20 category and this is the dataset where we only have a handful of examples per category we can actually use a different type of classifier that may be more effective in this case specifically some research has shown that cosigned or similarity based classifiers rather than fully connected linear layers might perform better on the left you see the standard linear layer where we have a set of weights and we want to perform a classification task that is output scores that are that fed through soft Max to probability we have a set of weights and these weights can be seen as something modulators the inputs that as we perform a linear weighted summation in order to calculate the scores on the right you see what the paper calls baseline plus plus which uses a different type of classifier namely a cosigned classifier here we're referring to cosign.</p><p><span class="timestamp">[00:04:00]</span>
Based similarity metric we still have a set of weights but we can interpret each weight vector as a set of prototypes that is what the classes might look like and we perform some similarity computation between these prototypes and the query and the output of that is also fed to a software specifically here's the cosigned similarity metric if you notice this is actually similar to adopt product where you're just multiplying the input times the weight Matrix or vector but you're normalizing this by the size of these vectors and so what you end up with is a unit norm comparison that is you're only looking at the angles between the feature vectors rather than incorporating how long they are across these different dimension and so this might actually provide better discrimination between a small number of classes because you are really focusing on just an angular difference rather than anything else again because of the fact that we're only discriminating across a small number of classes this end up working better than a normal linear letter and of course that has something to do with the way the few shot learning experiment was set up and so in other settings This may or may not work better there are several downsides to the fine tuning approach the training that we do on the base set of classes where we have lots of classes and lots of the data or examples per class all we perform is our normal optimization in order to do well on the large set of categories but this does not factor the task into account that we will encounter during query or inference time specifically there's no notion that will be performing a bunch of and wait tests that is will have a new set of categories where n. is often smaller from $5.00 to $20.00 typically and will have a new support set which are a small labeled.</p><p><span class="timestamp">[00:06:00]</span>
Examples for these and categories typically $1.00 to $5.00 examples per category and it will perform these and weigh tests during inference time and so this is a general principle where we would like to align what we do during training with what we do during testing with the idea that if we can optimize for it this will provide a much better generalization and you will perform much better during actual test time and so one idea is to just simulate what we will see during test time using the large label data set that we have this is called met a training will take our large base data set that has many categories say 100 and lots of examples per category and simulate a set of smaller tasks which better mirror what will happen during testing specifically the end way k. shot task where n. is the number of categories that's very small from 3 to 20 and k. is the number of examples per category which again is small typically somewhere from $1.00 to $5.00 in this case you see any close to 3 categories and k. close to 2 examples per category in the support set and so the 1st training task will randomly sample a set of 3 categories and 2 examples per category even though the base dataset actually has more and then we'll have a query set also sample the support set here is called The Met a train set and then the quavery is called The Met a test set and what will do is we'll have a model that forward propagates that is somehow learns something from the support set or extracts features from the supports and uses it to then make predictions on the queries and then we'll back propagate through that error if our predictions are wrong and we'll do this many times will simulate another training task too and again make predictions and back propagate through some model and then finally after we do this at the.</p><p><span class="timestamp">[00:08:00]</span>
And we have the actual test set where we can measure our performance on how well we do on the tests and you can also option only pre-trained feature on held out base classes because we have lots of classes let's say 100 we can take 50 of them trained across all 50 categories and all the label data available just to pre-trained features and then use the remaining 50 categories to perform this Mehta training again the testing stage is the same but with new classes the key thing here is that during testing again the classes that you see are completely different from anything you've seen in the training set and different than the Met a training and not a testing such as Well there's a question of what we can learn across these various better training tasks that we set up in order to then do well on a completely new set of categories where again we have in the test set a small number of examples and then we want to for Aquarius set map that each query element into the right class so here a few examples across various works in the literature of what you can possibly learn on the left you see essentially what's called the matching network which just learns a feature extractor such that when we feed to support set into those feature extractors and extract the features and then similarly extract features for the query set some and coded metric actually allows you to find the most similar class for each query item specifically here you can use a cosigned similarity or distance function and essentially make predictions for each query which support element or which class does it most closely relate to so here you're going to take the training task one the better training tasks one that we set up use the feature extractor to extract features from the support set and again this can be one example per category or many examples per cat agree but still you essentially all.</p><p><span class="timestamp">[00:10:00]</span>
So extract features from the query and find for each query item What is the closest supports element that it matches to and say that that query element is that class one of the end classes that we have and then you again take the training met a training task too and do the same thing over and over again during inference again you're set up with the same saying you have a new set of any classes and we have some support set that is a small number of labeled examples for those and classes and we'll use the same procedure where we extract features from both and find for each query item the closest support element you can also have a prototypical network showed in the middle here where rather than matching each query item to the support set you actually match it to a prototype or mean embedding that is let's say you have 2 examples for category in the support set then you take for each class the 2 examples you extract are features and then you take the mean of those and then you compare the each query item to that mean rather than each individual support item you can also have some generalizations of this where you have what's called a relationship network which rather than learning some feature I'm betting that is then compared using the hand coded similarity or distance function you can actually try to learn how to relate in a more complicated manner each quavery item to the set of support examples that you have in that the.</p><p><span class="timestamp">[00:11:30]</span>
If you think about it we can actually view this in a bit more general way specifically we can view it as parameterizing an actual learning algorithm itself that is we're taking the Mehta training task one we have a support set and we want to perform some learning on that supports it and then we want to perform some predictions on the query set now in the examples we've shown previously this is very simple because all we're learning is a feature x..</p><p><span class="timestamp">[00:12:00]</span>
Traktor such that when we feed to support inquiry sets we can use some hand coded similarity or distance function to figure out for each query element which one of the end classes it belongs to by matching it to the nearest support element however we can try to do this in a more general way we can actually perform some form of gradient descent or wait updates on a model on the support set and then make predictions on the query since now we've been using stochastic gradient descent to do that where we take the negative of the gradient multiplied by the learning rate to perform the weight updates on the support set and then we make predictions on the carry set at the end however this doesn't necessarily have to be the case so cast a gradient descent is well known to overfit if we train it on a small support set where we only have a few examples per capita so we could try to broaden the solution and actually learn some update rule that's not stochastic gradient descent but somehow is a function a learnable better learning function that essentially takes in the gradients and does some update on a model so we can have a more general view of learning how to learn and there are several ways to look at this both the examples that we've shown thus far and the examples we will show in the next few slides but this is in general called better learning because where learning to learn we can take inspiration from a known learning algorithm the examples that we showed can be fit into this categorization specifically the matching network where we extract some features and then we perform a one nearest neighbor that is we take the Querrey features and match it to the nearest support features and whatever class that ends up being that's the prediction we make for the query class this is actually the same thing as a k. nearest neighbor except it's a learner Will Cain nearest neighbor because we're learning to feature space which it also actually learned the metric that we use but in the cases that I showed we actually used hand coded metrics such as cosign distance or similarity function.</p><p><span class="timestamp">[00:14:00]</span>
The prototypical networks where you take let's say 2 to 3 examples for a particular class and you extract features and then you take the meaning of those features to get a prototype embedding can be seen as a Gaussian cost of Fire where we learn a galaxy and model a mean and we can also do variants for each class and then we essentially take each query item and match it to that cousine but you can actually have some more interesting learning to learn kind of setups specifically as I alluded to we could try to learn gradient descent itself namely the update rule there are several flavors of this one is called a metal or l.s.d. m. the use gradient descent where we take as inputs a set of gradients and we make some updates to the model as an ls t.m. similar to an l.s.t. m. we have some previous weights that we have and then we take as input gradients and then we output a new set of weights this can be again mapped to dissimilar ls d.m. computations that we learned about it will see this in the next few slides There's also a simpler form called Model at Gnostic better learning which actually takes the view of just learning an initialization that is we want to learn some initial set of weights that after performing gradient descent specifically stochastic gradient descent on the support set we don't over fit as much that as we can make better predictions on the query and so you're not really learning the update rule itself but you're learning initialization that gets fed through normal stochastic gradient the scent on the supports set which again is only a few examples per class and we're trying to learn some initialization that maybe after stochastic gradient the sent up the rules does it over fit as much there's other even more black box ways to look at a learning algorithm These are called Man and snail as examples but we won't cover them there rather complex and this is an evolving field so let's take a gradient the set now lets.</p><p><span class="timestamp">[00:16:00]</span>
Say that we actually want to learn how to perform gradient descent there's several parts to gradient the scent itself there is the parameter initialization that is what are the initial weights that we will use and then there's the update rules the output of this algorithm is parameter initialization at a metal learner that decides how to update the parameters when fed the support set or alternatively we can learn just in the civilization and use the normal gradient descent here the output is just a parameter initialization and then we feet this initial ization through gradient the scent on the support set and hope that the initial ization is good enough compared to let's say random initialization such that it doesn't over bit as much let's take a look at how to learn gradient descent itself that is train gradient the scent like procedure that can be applied to a model and order to update its weights given the support set if you remember gradient the scent starts from some initial parameters data 0 and performs the following update the value of beta which is the set of weights or parameters at time t. is equal to say that t. minus one minus Alpha learning great times the gradient what's interesting about this is that the gradient the sent up the rule actually looks quite similar to the l.s.d. m. cell State Update If you remember the l.s.d. I'm cell State Update rule is this follows the cell state at time t. is equal to some forget gate that we compute at time t. times the previous cell state see t. minus one plus some input gate calculated at time t. times some update value Tilda if you remember the intuition is that the Forget gait determines how much of the past cell state to forget and the implicate the term and how much of the new information to incorporate and we can also have.</p><p><span class="timestamp">[00:18:00]</span>
Which is the new information be computed as well at each time step here c.t. or the cell state if we map it to the gradient the scent equation is equivalent to the parameters they'd a t. that is the cell state is the actual model parameters that we want to update if you think about it we depict we have started with a 0 cell state but we can look at the initialization which we can actually back propagate all the way through to the initialization of the cell state and we can view it this as a learned initialization that is what weights should we start with such that when we perform this learnable gradient descent update procedure on the support set will perform the best on the query set the State Update value c t Tilda can actually be mapped to in the gradient the sent the equation to the negative gradient in this case it's not anything dramatizes something we compute again we can compute the gradients and we can try to then perform the update for get and implicates actually have some really cool interpretations if we view this procedure as a gradient descent that is the input gate is modulating how much of the negative gradient to actually update the weights this is just a learning rate but it's an adaptive learning rate because we compute the input gate according to the Ls t.m. equations conditioned on some information at each time step so it's an adaptive learning rate that's learned because we learn the weights that are used to compute this implicate Similarly the Forget gait can be seen as an adaptive weight the k. because it by July 8th how much we reduce the previous weights by it so if we select a high forgetting that is computed again you know learnable way that has weights then will essentially wipe out or reduce the values of previous weights and so we're decaying the weight if you look at this entire thing that is viewing gradient the center.</p><p><span class="timestamp">[00:20:00]</span>
As a learning algorithm and we map it to update rules which has several learnable elements they mainly the input and the forget as well as the initialization that were essentially learning how to do gradient descent again the key idea is that stochastic gradient descent which is shown on top here just taking the negative gradient multiply it by a learning rate and adding that to the previous weights has a tendency to over fit when trained on a supports that has a small number of examples per capita or the question is can we learn this gradient the set procedure that makes different types of updates and has an adaptive learning rate and adaptive weight the k. and can we do much better can we essentially overfit less when we apply this procedure to update the weights of our model on the supports Here's a graphical depiction of the same in this case we can actually see how up the weights across gradient descent is actually just a computation a graph and that means we can actually back propagate through this computation graph to update the weights of the meddler in this case analyst t.m. in order to optimize those weights which determine for example how to compute the book gate or the Forget gate as well as what initialization to start with we begin with data 0 which again can be a learned set of initial weights that we start with that is we can back propagate all the way back to say to 0 and update they don't 0 itself again in normal L.S.D.'s we often start with all the rows but in this case we actually want to learn initialization and for each mini batch of data we take the previous parameters and output a new set of parameters in this case will have x. one y. one be the 1st mini batch again this is a few shot learning so the support set that is the entire set of label data that we have might actually fit into.</p><p><span class="timestamp">[00:22:00]</span>
Emory because it's such a small amount in that case x. one x. 2 and so on are all the same we're applying the iterative algorithm to it over and over again just like we do in gradient descent or x. one x. 2 could be different if you can actually fit entire supports in memory regardless we take the initialization they does the row and the many batch x. one y. one and compute degrading the input to our metal learner is this initialization as well as the gradient and then it computes a new set of parameters they don't want this they don't want again is used to compute the gradients for a new mini batch x. to y. to where we have as a result the gradients and those gradients are fed to the metal learner which takes state to one the previous parameters and then updates them using these grading and so on and so we can actually view this as a computation graph where we're taking the many batches of data and initialization and progressively are iteratively updating the parameters at the end we have a new set of parameters the final set of parameters and we just apply them to the test set to get our results now because we perform the Mediterranean procedure where we have a mega training that is the support set that were performing just learned great in the sense update procedure on and the Met a test which is the query set we can actually then make predictions on the query set and then have a last function because we actually have the labels for those and back propagate through this entire chain so we're literally back propagating through gradient the sent in order to update the weights of the Ls d.m. that is again the weights that determine the input values at each time step the Forget gate values as well as the they does the row which is the initialization this l.s.t. of viewpoint is actually quite general were.</p><p><span class="timestamp">[00:24:00]</span>
Actually learning gradient descent procedure are it's not clear that all this machinery is necessary specifically there are other competing algorithms that learn just one part of this which is the initialization that is rather than changing all of the steepest gradient the set up the rules that we've been using will actually still continue to use it but will back propagate through that gradient the set procedure in order to learn and initialisation that is we're asking the question what additional ization is a robust to the standard steepest gradient procedure would apply to a small number of examples such that it doesn't over fit and can generalize to the query set this is called mammal or model ignores the better learning again where using the same general recipe will take our large label data set blended into this matter training set where we have some training and some tests and then we'll apply a gradient the scent for let's say the 1st row here the 1st part of the training set and then we'll essentially measure our performance on the test set and that back propagate through that gradient descent that we applied on the training data and will back propagate all the way through to the initialization and so we're updating the additional ization based on the training set and applying the normal steepest gradient the scent on it all the way to the test a data where we make predictions and get a loss and so on that will go to the next episode where we take the 2nd row here of data apply a gradient descent steepest gradient the scent on the training data measure our performance on the test set get a loss and back propagate So again this is the same thing that we've been doing except the key idea is that we're only learning the initialisation we're not learning how to actually perform the updates given the gradients as a learned of all function all we're doing is back propagating all the way to the in the shell as Asian and updating it so we're trying to learn that in.</p><p><span class="timestamp">[00:26:00]</span>
That's robust to applying steepest gradient the set to a small number of examples and that can then generalize to the test again one of the key reasons that this is a good idea is that we can actually view gradient descent as a computation graph that is the procedure of taking the previous premolars computing the gradients on a mini batch of data and then updating the parameters is a set of computation and doing this iteratively creates a long computation graph that is sequence and we can then back propagate through this entire thing either learning parameters of the update rules themselves as in the case of l s t M's or just updating the initialization back propagating this in through the entire stochastic gradient the set set of iterations all the way back to the initial ization of course there's a lot of complexity here that we haven't gone through and also you can only do this through some number of as g.d. iterations of course doing a back propagation across thousands or millions of bit or ations is not feasible and so typically what we do is back propagate only through let's say 4 to 10 year to ration this is actually not bad because again we only have a small amount the label data anyways and so we'll be doing several epochs by that anyways there are several tradeoffs between the complexities of these 2 types of algorithms the Arnette or l.s.d. based metal learning is obviously much more flexible it allows us to actually learn the update rules themselves but this may incur some complexity and some consequences in terms of convergence of the algorithm what it converges to and whether actually performs well mammal which is back propagating only to the national zation actually does have nicer properties because we're not actually changing to up the rules so we know that the procedure will converge and we're not sure what it converges to but.</p><p><span class="timestamp">[00:28:00]</span>
Though it converges to some local optimum just like normal gradient descent again all we're changing is the initialization and if it's not good enough we can actually do many more gradient descent updates steps and so this is actually some trade off between these 2 algorithms namely the complexity or flexibility of what we're learning as well as the results on things like Convergence and effect of performance given that you're performing that's because of the very simple nature of mammal This is actually much more dominant these days so in summary what we've done here is taken a small amount of label data in our actual query time or inference time and are able to perform some update rules on the support set in order to learn a model that does well on the query set and we did this by performing this Mediterranean procedure that is taking the large label dataset that we had with 100 categories and many many examples per category and the centrally split up into many smaller tasks that we are back propagating through now again this is an evolving field and so there's lots of competing papers that do or don't use metal learning and even some recent methods that use things like cosigned distance based classifiers are just better pre-training and fine tuning actually can beat better learning in many cases however what's nice is that mammal and similar measure learning algorithms are very general and flexible and so they can be applied not just to few shot learning but to other things like unsupervised learning and so on.</p><p></p>
          </div>
        </div>
      
    </div>
  </div>
  <script src="../js/script.js"></script>
</body>
</html>